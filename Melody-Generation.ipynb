{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWReWjdRjQ6P"
      },
      "source": [
        "**Music Melody-Conditional Multitrack Generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfcYQM4qjTis"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import pretty_midi\n",
        "import pypianoroll\n",
        "import tables\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "import music21\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "import json\n",
        "import IPython.display\n",
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import random\n",
        "import itertools\n",
        "root_dir = 'drive/MyDrive/ProjectMusic\n",
        "data_dir = root_dir + '/Lakh Piano Dataset/LPD-5/lpd_5/lpd_5_cleansed'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMex7xeZoaor",
        "outputId": "a2133b73-b56b-43b4-da3a-b41bc3699773"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -qU pyfluidsynth pretty_midi\n",
        "!pip install music21\n",
        "!pip install pypianoroll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puztPxIyh3oV",
        "outputId": "ab0424f8-2c46-42bb-a2d0-86537ee7c086"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhR5l1SjNTa5"
      },
      "source": [
        "**Getting MIDI and Song Metadata**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng13lLDrN8Za"
      },
      "outputs": [],
      "source": [
        "RESULTS_PATH = os.path.join(root_dir, 'Lakh Piano Dataset', 'Metadata')\n",
        "\n",
        "# Utility functions for retrieving paths\n",
        "def msd_id_to_dirs(msd_id):\n",
        "    \"\"\"Given an MSD ID, generate the path prefix.\n",
        "    E.g. TRABCD12345678 -> A/B/C/TRABCD12345678\"\"\"\n",
        "    return os.path.join(msd_id[2], msd_id[3], msd_id[4], msd_id)\n",
        "\n",
        "\n",
        "def msd_id_to_h5(msd_id):\n",
        "    \"\"\"Given an MSD ID, return the path to the corresponding h5\"\"\"\n",
        "    return os.path.join(RESULTS_PATH, 'lmd_matched_h5',\n",
        "                        msd_id_to_dirs(msd_id) + '.h5')\n",
        "\n",
        "# Load the midi npz file from the LMD cleansed folder\n",
        "def get_midi_npz_path(msd_id, midi_md5):\n",
        "    return os.path.join(data_dir,\n",
        "                        msd_id_to_dirs(msd_id), midi_md5 + '.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY-AyplDYXSm",
        "outputId": "76c32d84-a421-485f-8a08-c34927584310"
      },
      "outputs": [],
      "source": [
        "# Open the cleansed ids - cleansed file ids : msd ids\n",
        "cleansed_ids = pd.read_csv(os.path.join(root_dir, 'Lakh Piano Dataset', 'cleansed_ids.txt'), delimiter = '    ', header = None)\n",
        "lpd_to_msd_ids = {a:b for a, b in zip(cleansed_ids[0], cleansed_ids[1])}\n",
        "msd_to_lpd_ids = {a:b for a, b in zip(cleansed_ids[1], cleansed_ids[0])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGhaBVSOzweZ"
      },
      "outputs": [],
      "source": [
        "# Reading the genre annotations\n",
        "genre_file_dir = os.path.join(root_dir, 'Lakh Piano Dataset', 'Genre', 'msd_tagtraum_cd1.cls')\n",
        "ids = []\n",
        "genres = []\n",
        "with open(genre_file_dir) as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "        if line[0] != '#':\n",
        "          split = line.strip().split(\"\\t\")\n",
        "          if len(split) == 2:\n",
        "            ids.append(split[0])\n",
        "            genres.append(split[1])\n",
        "          elif len(split) == 3:\n",
        "            ids.append(split[0])\n",
        "            ids.append(split[0])\n",
        "            genres.append(split[1])\n",
        "            genres.append(split[2])\n",
        "        line = f.readline()\n",
        "genre_df = pd.DataFrame(data={\"TrackID\": ids, \"Genre\": genres})\n",
        "\n",
        "genre_dict = genre_df.groupby('TrackID')['Genre'].apply(lambda x: x.tolist()).to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Le14xf4NVd"
      },
      "source": [
        "**Objects that we need**\n",
        "\n",
        "- cleansed_ids: dictionary of LPD file name : MSD file name\n",
        "- lmd_metadata: list of dictionaries - each dict has a msd_id field to identify\n",
        "- Get the lmd_file_name (actual path )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOeVzvnwV2ht"
      },
      "outputs": [],
      "source": [
        "# Load the processed metadata\n",
        "with open(os.path.join(root_dir, 'Lakh Piano Dataset', 'processed_metadata.json'), 'r') as outfile:\n",
        "  lmd_metadata = json.load(outfile)\n",
        "\n",
        "# Change this into a dictionary of MSD_ID: metadata\n",
        "lmd_metadata = {e['msd_id']:e for e in lmd_metadata}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU0th4Rt5ZMu"
      },
      "outputs": [],
      "source": [
        "# Get all song MSD IDs in pop rock genre\n",
        "metal_song_msd_ids = [k for k, v in lmd_metadata.items() if 'rock' in v['artist_terms']]\n",
        "\n",
        "# Randomly choose 1000 songs out of these\n",
        "train_ids = random.choices(metal_song_msd_ids, k = 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xM2flv96PUg"
      },
      "outputs": [],
      "source": [
        "combined_pianorolls = []\n",
        "i = 0\n",
        "for msd_file_name in train_ids:\n",
        "\n",
        "  lpd_file_name = msd_to_lpd_ids[msd_file_name]\n",
        "  # Get the NPZ path\n",
        "  npz_path = get_midi_npz_path(msd_file_name, lpd_file_name)\n",
        "  multitrack = pypianoroll.load(npz_path)\n",
        "  multitrack.set_resolution(2).pad_to_same()\n",
        "\n",
        "  # Piano, Guitar, Bass, Strings, Drums\n",
        "  # Splitting into different parts\n",
        "\n",
        "  parts = {'piano_part': None, 'guitar_part': None, 'bass_part': None, 'strings_part': None, 'drums_part': None}\n",
        "  song_length = None\n",
        "  empty_array = None\n",
        "  has_empty_parts = False\n",
        "  for track in multitrack.tracks:\n",
        "    if track.name == 'Drums':\n",
        "      parts['drums_part'] = track.pianoroll\n",
        "    if track.name == 'Piano':\n",
        "      parts['piano_part'] = track.pianoroll\n",
        "    if track.name == 'Guitar':\n",
        "      parts['guitar_part'] = track.pianoroll\n",
        "    if track.name == 'Bass':\n",
        "      parts['bass_part'] = track.pianoroll\n",
        "    if track.name == 'Strings':\n",
        "      parts['strings_part'] = track.pianoroll\n",
        "    if track.pianoroll.shape[0] > 0:\n",
        "      empty_array = np.zeros_like(track.pianoroll)\n",
        "\n",
        "  for k,v in parts.items():\n",
        "    if v.shape[0] == 0:\n",
        "      parts[k] = empty_array.copy()\n",
        "      has_empty_parts = True\n",
        "\n",
        "  # Stack all together - Piano, Guitar, Bass, Strings, Drums\n",
        "  combined_pianoroll = torch.tensor([parts['piano_part'], parts['guitar_part'], parts['bass_part'], parts['strings_part'], parts['drums_part']])\n",
        "\n",
        "  # These contain velocity information - the force with which the notes are hit - which can be standardized to 0/1 if we want (to compress)\n",
        "  if has_empty_parts == False:\n",
        "    combined_pianorolls.append(combined_pianoroll)\n",
        "    i+=1\n",
        "    print(i)\n",
        "\n",
        "  if i == 1000:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TeycNi6YyCm"
      },
      "outputs": [],
      "source": [
        "pianoroll_lengths = [e.size()[1] for e in combined_pianorolls]\n",
        "combined_pianorolls = torch.hstack(combined_pianorolls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH0KCy1XZFC9"
      },
      "outputs": [],
      "source": [
        "torch.save(combined_pianorolls, os.path.join(root_dir, 'Lakh Piano Dataset', 'metal_1000_pianorolls.pt'))\n",
        "pianoroll_lengths = torch.tensor(pianoroll_lengths)\n",
        "torch.save(pianoroll_lengths, os.path.join(root_dir, 'Lakh Piano Dataset', 'metal_1000_pianorolls_lengths.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wd2LnhQ-z7w"
      },
      "outputs": [],
      "source": [
        "# Loading\n",
        "combined_pianorolls = torch.load(os.path.join(root_dir, 'Lakh Piano Dataset', 'rock_1000_pianorolls_res2.pt'))\n",
        "pianoroll_lengths = torch.load(os.path.join(root_dir, 'Lakh Piano Dataset', 'rock_1000_pianorolls_res2_lengths.pt'))\n",
        "pianoroll_lengths = pianoroll_lengths.numpy()\n",
        "pianoroll_cum_lengths = pianoroll_lengths.cumsum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n49U3aXeavYV"
      },
      "outputs": [],
      "source": [
        "# Normalize\n",
        "combined_pianorolls = combined_pianorolls / 127.0\n",
        "\n",
        "# ### Getting the number of notes played in that time step\n",
        "# # Number of notes per time step per track\n",
        "# notes_per_time_step = (combined_pianorolls > 0).type(torch.float32).sum(axis = 2)\n",
        "# # Censor those with more than 10 notes to be 10\n",
        "# notes_per_time_step[notes_per_time_step > 10] = 10\n",
        "# # Normalize to be between [0, 4] - very important to get right\n",
        "# notes_per_time_step = notes_per_time_step / 2\n",
        "# notes_per_time_step = notes_per_time_step.unsqueeze(2)\n",
        "# # Concatenate the number vector\n",
        "# combined_pianorolls = torch.cat((combined_pianorolls, notes_per_time_step), dim = 2)\n",
        "\n",
        "# Remake the list of pianorolls - ensuring all songs are multiple of 32\n",
        "pianorolls_list = []\n",
        "pianorolls_list.append(combined_pianorolls[:, :(pianoroll_cum_lengths[0] - pianoroll_cum_lengths[0] % 32), :])\n",
        "for i in range(len(pianoroll_cum_lengths) - 1):\n",
        "  length = pianoroll_cum_lengths[i+1] - pianoroll_cum_lengths[i]\n",
        "  # Get the nearest multiple of 32\n",
        "  length_multiple = length - (length % 32)\n",
        "  pianoroll = combined_pianorolls[:, pianoroll_cum_lengths[i]:(pianoroll_cum_lengths[i] + length_multiple), :]\n",
        "  pianorolls_list.append(pianoroll)\n",
        "\n",
        "# Combine the pianorolls again\n",
        "combined_pianorolls = torch.hstack(pianorolls_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r82cg5RvbJSy"
      },
      "source": [
        "**Creating Music Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3lZ7cHoGMcW"
      },
      "outputs": [],
      "source": [
        "# Creating dataset and dataloader\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGvqsItLwYR6"
      },
      "outputs": [],
      "source": [
        "# Dataset which only returns sequences which are multiples of 32\n",
        "class CombinedDataset(Dataset):\n",
        "  def __init__(self, pianorolls, instrument_id):\n",
        "    self.data = pianorolls\n",
        "    self.length = int(pianorolls.size(1) / 32)\n",
        "    self.instrument_id = instrument_id\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sequence = self.data[self.instrument_id, (index * 32):((index+1) * 32), :]\n",
        "    return sequence\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RgV2FntJ3SQ"
      },
      "outputs": [],
      "source": [
        "# Generation Dataset - all fields cannot be blank\n",
        "class GenerationDataset(Dataset):\n",
        "  def __init__(self, list_of_sequences, dataset_length = 32 * 10000, seq_length = 50):\n",
        "\n",
        "    # Don't normalize anymore since it was done earlier\n",
        "    self.data = list_of_sequences\n",
        "    self.n_songs = len(list_of_sequences)\n",
        "    self.seq_length = seq_length\n",
        "    self.length = dataset_length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    valid_sequence = False\n",
        "    while valid_sequence == False:\n",
        "      # Choose a random song id\n",
        "      song_id = random.randint(0, self.n_songs - 1)\n",
        "      song_length = self.data[song_id].size()[1]\n",
        "      # Choose a random start window\n",
        "      start_time = random.randint(0, song_length - self.seq_length * 2 - 2)\n",
        "      start_time = start_time - (start_time % 32)\n",
        "      # Check that every track is not empty\n",
        "      piano_sequence = self.data[song_id][0, start_time:(start_time + self.seq_length), :]\n",
        "      guitar_sequence = self.data[song_id][1, start_time:(start_time + self.seq_length), :]\n",
        "      bass_sequence = self.data[song_id][2, start_time:(start_time + self.seq_length), :]\n",
        "      strings_sequence = self.data[song_id][3, start_time:(start_time + self.seq_length), :]\n",
        "      drums_sequence = self.data[song_id][4, start_time:(start_time + self.seq_length), :]\n",
        "\n",
        "      if piano_sequence.sum() != 0 and guitar_sequence.sum() != 0 and bass_sequence.sum() != 0 \\\n",
        "      and strings_sequence.sum() != 0 and drums_sequence.sum() != 0:\n",
        "        valid_sequence = True\n",
        "      else:\n",
        "        if random.random() < 0.1:\n",
        "          valid_sequence = True\n",
        "\n",
        "    train_sequence = self.data[song_id][:, start_time:(start_time + self.seq_length), :]\n",
        "    target_sequence = self.data[song_id][:, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "    return train_sequence, target_sequence\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6naC4urRFUqP"
      },
      "outputs": [],
      "source": [
        "# Melody prediction dataset - predict the next melody given the current melody\n",
        "class MelodyDataset(Dataset):\n",
        "  def __init__(self, list_of_sequences, dataset_length = 32 * 10000, seq_length = 50):\n",
        "\n",
        "    # Don't normalize anymore since it was done earlier\n",
        "    self.data = list_of_sequences\n",
        "    self.n_songs = len(list_of_sequences)\n",
        "    self.seq_length = seq_length\n",
        "    self.length = dataset_length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Choose a random song id\n",
        "    song_id = random.randint(0, self.n_songs - 1)\n",
        "    song_length = self.data[song_id].size()[1]\n",
        "    # Choose a random start window\n",
        "    start_time = random.randint(0, song_length - self.seq_length * 2 - 2)\n",
        "    # train_sequence: 1 (piano) x seq_length x 128\n",
        "    train_sequence = self.data[song_id][0, start_time:(start_time + self.seq_length), :]\n",
        "    # target_sequence: 1 (piano) x seq_length x 128\n",
        "    target_sequence = self.data[song_id][0, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "    return train_sequence, target_sequence\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "# Melody-conditional dataset NEW - returns BOTH the previous harmony, and current melody, and current harmony\n",
        "# only outputs samples with all tracks non-empty\n",
        "class ConditionalDataset(Dataset):\n",
        "  def __init__(self, list_of_sequences, dataset_length = 32 * 1000, seq_length = 50, instrument = 'guitar'):\n",
        "\n",
        "    self.data = list_of_sequences\n",
        "    self.n_songs = len(list_of_sequences)\n",
        "    self.seq_length = seq_length\n",
        "    self.length = dataset_length\n",
        "    self.instrument = instrument\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Choose a random song id\n",
        "    valid_sequence = False\n",
        "\n",
        "    while valid_sequence == False:\n",
        "      song_id = random.randint(0, self.n_songs - 1)\n",
        "      song_length = self.data[song_id].size()[1]\n",
        "\n",
        "      # Choose a random start window\n",
        "      start_time = random.randint(0, song_length - self.seq_length * 2 - 2)\n",
        "\n",
        "      # train_sequence: 1 (piano) x seq_length x 128\n",
        "      piano_sequence = self.data[song_id][0, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "\n",
        "      if self.instrument == 'guitar':\n",
        "        past_sequence = self.data[song_id][1, start_time:(start_time + self.seq_length), :]\n",
        "        target_sequence = self.data[song_id][1, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "      elif self.instrument == 'bass':\n",
        "        past_sequence = self.data[song_id][2, start_time:(start_time + self.seq_length), :]\n",
        "        target_sequence = self.data[song_id][2, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "      elif self.instrument == 'strings':\n",
        "        past_sequence = self.data[song_id][3, start_time:(start_time + self.seq_length), :]\n",
        "        target_sequence = self.data[song_id][3, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "      elif self.instrument == 'drums':\n",
        "        past_sequence = self.data[song_id][4, start_time:(start_time + self.seq_length), :]\n",
        "        target_sequence = self.data[song_id][4, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "      else:\n",
        "        past_sequence = None\n",
        "        target_sequence = None\n",
        "\n",
        "      if piano_sequence.sum() != 0 and past_sequence.sum() != 0 and target_sequence.sum() != 0:\n",
        "        valid_sequence = True\n",
        "      else:\n",
        "        if random.random() < 0.1:\n",
        "          valid_sequence = True\n",
        "\n",
        "\n",
        "    return piano_sequence, past_sequence, target_sequence\n",
        "  def __len__(self):\n",
        "    return self.length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pZxOyk_NGTCE"
      },
      "outputs": [],
      "source": [
        "# @title Old Datasets\n",
        "# Melody prediction dataset - target is next ONE time period\n",
        "class MelodyDatasetOld(Dataset):\n",
        "  def __init__(self, list_of_sequences, dataset_length = 32 * 10000, seq_length = 50):\n",
        "\n",
        "    # Don't normalize anymore since it was done earlier\n",
        "\n",
        "    self.data = list_of_sequences\n",
        "    self.n_songs = len(list_of_sequences)\n",
        "    self.seq_length = seq_length\n",
        "    self.length = dataset_length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Choose a random song id\n",
        "    song_id = random.randint(0, self.n_songs - 1)\n",
        "    song_length = self.data[song_id].size()[1]\n",
        "\n",
        "    # Choose a random start window\n",
        "    start_time = random.randint(0, song_length - self.seq_length - 2)\n",
        "\n",
        "    # train_sequence: 5 x seq_length x 128\n",
        "    train_sequence = self.data[song_id][:, start_time:(start_time + self.seq_length), :]\n",
        "\n",
        "    # target_sequence: 1 (piano) x 1 x 128\n",
        "    target_sequence = self.data[song_id][0, (start_time + self.seq_length + 1), :]\n",
        "    return train_sequence, target_sequence\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "\n",
        "# Melody-conditional dataset - only outputs samples with all tracks non-empty\n",
        "class ConditionalDatasetOld(Dataset):\n",
        "  def __init__(self, list_of_sequences, dataset_length = 32 * 1000, seq_length = 50, instrument = 'guitar'):\n",
        "\n",
        "    self.data = list_of_sequences\n",
        "    self.n_songs = len(list_of_sequences)\n",
        "    self.seq_length = seq_length\n",
        "    self.length = dataset_length\n",
        "    self.instrument = instrument\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Choose a random song id\n",
        "    valid_sequence = False\n",
        "\n",
        "    while valid_sequence == False:\n",
        "      song_id = random.randint(0, self.n_songs - 1)\n",
        "      song_length = self.data[song_id].size()[1]\n",
        "\n",
        "      # Choose a random start window\n",
        "      start_time = random.randint(0, song_length - self.seq_length - 2)\n",
        "\n",
        "      # train_sequence: 1 (piano) x seq_length x 128\n",
        "      piano_sequence = self.data[song_id][0, start_time:(start_time + self.seq_length), :]\n",
        "\n",
        "      if self.instrument == 'guitar':\n",
        "        target_sequence = self.data[song_id][1, start_time:(start_time + self.seq_length), :]\n",
        "      elif self.instrument == 'bass':\n",
        "        target_sequence = self.data[song_id][2, start_time:(start_time + self.seq_length), :]\n",
        "      elif self.instrument == 'strings':\n",
        "        target_sequence = self.data[song_id][3, start_time:(start_time + self.seq_length), :]\n",
        "      elif self.instrument == 'drums':\n",
        "        target_sequence = self.data[song_id][4, start_time:(start_time + self.seq_length), :]\n",
        "      else:\n",
        "        target_sequence = None\n",
        "\n",
        "      if piano_sequence.sum() != 0 and target_sequence.sum() != 0:\n",
        "        valid_sequence = True\n",
        "\n",
        "    return piano_sequence, target_sequence\n",
        "  def __len__(self):\n",
        "    return self.length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA16WwkYC9sp"
      },
      "source": [
        "**RNN Generation Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dmb9CsXdPges"
      },
      "outputs": [],
      "source": [
        "# @title Encoder-Decoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size = 128, hidden_size = 64, num_layers = 1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.gru = nn.GRU(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)\n",
        "    def forward(self, input):\n",
        "        # Input: batch_size x seq_length x n_pitches\n",
        "        input = input.permute(1,0,2)\n",
        "        output, state = self.gru(input)\n",
        "\n",
        "        # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "        # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "        return output, state\n",
        "\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, enc_X, dec_X):\n",
        "        enc_outputs = self.encoder(enc_X)\n",
        "        dec_state = self.decoder.init_state(enc_outputs)\n",
        "        return self.decoder(dec_X, dec_state)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.gru = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n",
        "                          dropout=dropout)\n",
        "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
        "        \n",
        "    def init_state(self, enc_outputs):\n",
        "        return enc_outputs[1]\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        # The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)\n",
        "        X = self.embedding(X).permute(1,0,2)\n",
        "        # Broadcast `context` so it has the same `num_steps` as `X`\n",
        "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
        "        \n",
        "        # Concatenate X and context \n",
        "        X_and_context = torch.cat((X, context), 2)\n",
        "        \n",
        "        # Recurrent unit\n",
        "        output, state = self.gru(X_and_context, state)\n",
        "        \n",
        "        # Linear layer\n",
        "        output = self.dense(output).permute(1,0,2)\n",
        "\n",
        "        # `output` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "        # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "        return output, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "srgsyYUrPgi0"
      },
      "outputs": [],
      "source": [
        "# @title Time Distributed Layer\n",
        "class TimeDistributed(nn.Module):\n",
        "    def __init__(self, module, batch_first=False):\n",
        "        super(TimeDistributed, self).__init__()\n",
        "        self.module = module\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if len(x.size()) <= 2:\n",
        "            return self.module(x)\n",
        "\n",
        "        # Squash samples and timesteps into a single axis\n",
        "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
        "\n",
        "        y = self.module(x_reshape)\n",
        "\n",
        "        # We have to reshape Y\n",
        "        if self.batch_first:\n",
        "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
        "        else:\n",
        "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLlN_wLHImuA"
      },
      "outputs": [],
      "source": [
        "# ConditionalCNN - uses current melody and previous harmony to predict next harmony\n",
        "class ConditionalCNN(nn.Module):\n",
        "    def __init__(self, latent_size = 64):\n",
        "        super(ConditionalCNN, self).__init__()\n",
        "\n",
        "        # Encoding layers\n",
        "        self.enc_conv1 = nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = (4, 4), stride = (4, 4))\n",
        "        self.enc_conv2 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = (4, 4), stride = (4, 4))\n",
        "        self.enc_conv3 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = (2, 8), stride = (2, 8))\n",
        "        self.enc_lin1 = nn.Linear(512, 256)\n",
        "        self.enc_lin2 = nn.Linear(256, latent_size)\n",
        "\n",
        "        # Decoding layers\n",
        "        self.dec_lin = nn.Linear(latent_size, 256)\n",
        "        self.dec_conv1 = nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = (2, 8), stride = (2, 8))\n",
        "        self.dec_conv2 = nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = (4, 4), stride = (4, 4))\n",
        "        self.dec_conv3 = nn.ConvTranspose2d(in_channels = 64, out_channels = 1, kernel_size = (4, 4), stride = (4, 4))\n",
        "\n",
        "        self.batch_norm_2d64 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm_2d128 = nn.BatchNorm2d(128)\n",
        "        self.batch_norm_2d256 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, prev_harmony, melody):\n",
        "        # Input: batch_size x seq_length x n_pitches \n",
        "        prev_harmony = prev_harmony.unsqueeze(1)\n",
        "        # batch_size x num_channels (1) x seq_length x n_pitches \n",
        "        prev_harmony = F.relu(self.batch_norm_2d64(self.enc_conv1(prev_harmony)))\n",
        "        prev_harmony = self.dropout(prev_harmony)\n",
        "        prev_harmony = F.relu(self.batch_norm_2d128(self.enc_conv2(prev_harmony)))\n",
        "        prev_harmony = self.dropout(prev_harmony)\n",
        "        prev_harmony = F.relu(self.batch_norm_2d256(self.enc_conv3(prev_harmony)))\n",
        "        prev_harmony = prev_harmony.squeeze(3).squeeze(2)\n",
        "\n",
        "        melody = melody.unsqueeze(1)\n",
        "        # batch_size x num_channels (1) x seq_length x n_pitches \n",
        "        melody = F.relu(self.batch_norm_2d64(self.enc_conv1(melody)))\n",
        "        melody = self.dropout(melody)\n",
        "        melody = F.relu(self.batch_norm_2d128(self.enc_conv2(melody)))\n",
        "        melody = self.dropout(melody)\n",
        "        melody = F.relu(self.batch_norm_2d256(self.enc_conv3(melody)))\n",
        "        melody = melody.squeeze(3).squeeze(2)\n",
        "        \n",
        "        # Concat melody and previous harmony together\n",
        "        x = torch.cat((prev_harmony, melody), dim = 1)\n",
        "        x = F.relu(self.enc_lin1(x))\n",
        "        latent = self.enc_lin2(x)\n",
        "        x = F.relu(self.dec_lin(latent))\n",
        "        x = x.unsqueeze(2).unsqueeze(3)\n",
        "        x = F.relu(self.batch_norm_2d128(self.dec_conv1(x)))\n",
        "        x = F.relu(self.batch_norm_2d64(self.dec_conv2(x)))\n",
        "        x = F.relu(self.dec_conv3(x))\n",
        "        x = x.squeeze()\n",
        "        return x, latent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44OQGmqPVQKs"
      },
      "outputs": [],
      "source": [
        "# MelodyCNN - uses previous melody to predict next melody\n",
        "class MelodyCNN(nn.Module):\n",
        "    def __init__(self, latent_size = 64):\n",
        "        super(MelodyCNN, self).__init__()\n",
        "\n",
        "        # Encoding layers\n",
        "        self.enc_conv1 = nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = (4, 4), stride = (4, 4))\n",
        "        self.enc_conv2 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = (4, 4), stride = (4, 4))\n",
        "        self.enc_conv3 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = (2, 8), stride = (2, 8))\n",
        "        self.enc_lin = nn.Linear(256, latent_size)\n",
        "\n",
        "        # Decoding layers\n",
        "        self.dec_lin = nn.Linear(latent_size, 256)\n",
        "        self.dec_conv1 = nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = (2, 8), stride = (2, 8))\n",
        "        self.dec_conv2 = nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = (4, 4), stride = (4, 4))\n",
        "        self.dec_conv3 = nn.ConvTranspose2d(in_channels = 64, out_channels = 1, kernel_size = (4, 4), stride = (4, 4))\n",
        "\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.batch_norm_2d64 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm_2d128 = nn.BatchNorm2d(128)\n",
        "        self.batch_norm_2d256 = nn.BatchNorm2d(256)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Input: batch_size x seq_length x n_pitches \n",
        "        input = input.unsqueeze(1)\n",
        "        # batch_size x num_channels (1) x seq_length x n_pitches \n",
        "        x = F.relu(self.batch_norm_2d64(self.enc_conv1(input)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.batch_norm_2d128(self.enc_conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.batch_norm_2d256(self.enc_conv3(x)))\n",
        "\n",
        "        x = x.squeeze(3).squeeze(2)\n",
        "        latent = self.enc_lin(x)\n",
        "        x = F.relu(self.dec_lin(latent))\n",
        "        x = x.unsqueeze(2).unsqueeze(3)\n",
        "        x = F.relu(self.batch_norm_2d128(self.dec_conv1(x)))\n",
        "        x = F.relu(self.batch_norm_2d64(self.dec_conv2(x)))\n",
        "        x = F.relu(self.dec_conv3(x))\n",
        "        x = x.squeeze()\n",
        "        return x, latent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Yuq2N9GXVNBg"
      },
      "outputs": [],
      "source": [
        "# @title Old MelodyRNN/CNN\n",
        "# Outputs batch_size x 128 in the domain of [0, 1] - after sigmoid\n",
        "class MelodyRNNOld(nn.Module):\n",
        "  # input_size: number of possible pitches\n",
        "  # hidden_size: embedding size of each pitch\n",
        "  # output_size: number of possible pitches (probability distribution)\n",
        "    def __init__(self, hidden_size, input_size = 128 * 5, output_size = 128, batch_size = 32, n_layers=1):\n",
        "        super(MelodyRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.gru = nn.GRU(input_size, hidden_size, n_layers)\n",
        "        self.linear = nn.Linear(hidden_size * n_layers, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        # Can use convolutions in the future\n",
        "        # Input: batch_size x n_tracks x seq_length x pitches\n",
        "        input = input.permute(0,2,1,3)\n",
        "        # batch x seq_length x track x pitches\n",
        "        input = input.flatten(2,3) # Flatten the track and pitches together\n",
        "        # batch x seq_length x (track x pitches)\n",
        "        # # batch x seq_length x hidden_dim\n",
        "        input = input.permute(1,0,2)\n",
        "        # seq length x batch x hidden_dim\n",
        "        _, hidden = self.gru(input, hidden)\n",
        "\n",
        "        # Hidden: hidden layer at FINAL state\n",
        "        # hidden dim: (num_layer x num_dir) x batch x hidden_size\n",
        "        h_n = hidden.permute(1,0,2)\n",
        "        # h_n is batch x (num_layer x num_dir) x hidden_size\n",
        "        h_n = h_n.contiguous().flatten(1,2)\n",
        "        # After flattening: batch x (num_layer x num_dir x hidden_size)\n",
        "        output = self.linear(h_n)\n",
        "        output = torch.sigmoid(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device)\n",
        "\n",
        "# Outputs batch_size x 128 in the domain of [0, 1] - after sigmoid\n",
        "class MelodyCNNOld(nn.Module):\n",
        "    def __init__(self, seq_length = 32):\n",
        "        super(MelodyCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels = 5, out_channels = 64, kernel_size = (seq_length, 1), stride = (seq_length, 1))\n",
        "        self.conv2 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = (1, 8), stride = (1, 8))\n",
        "        self.conv3 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = (1, 16), stride = (1, 16))\n",
        "        self.linear = nn.Linear(256, 256)\n",
        "        self.out = nn.Linear(256, 128)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Can use convolutions in the future\n",
        "        # Input: batch_size x n_tracks x seq_length x pitches (32 x 5 x seq_length x 128)\n",
        "        x = F.relu(self.conv1(input)) # 32 x 5 x 1 x 128\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = x.squeeze()\n",
        "        x = F.relu(self.linear(x))\n",
        "        out = torch.sigmoid(self.out(x))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_7gmx5DPbzA"
      },
      "outputs": [],
      "source": [
        "def grad_clipping(net, theta):  \n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "    \n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad[:] *= theta / norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbvitNzSuq7J"
      },
      "outputs": [],
      "source": [
        "# Code to train all NNs - model_type: cond_cnn, melody_rnn, melody_cnn\n",
        "\n",
        "def run_epoch(dataloader, model, optimizer, criterion, is_train = True, model_type = 'melody_cnn'):\n",
        "  \n",
        "    running_loss = 0\n",
        "    n_obs = 0\n",
        "\n",
        "    if model_type == 'melody_cnn':\n",
        "      for train_seq, target_seq in dataloader:\n",
        "        train_seq = train_seq.to(device)\n",
        "        target_seq = target_seq.to(device)\n",
        "        output, latent = model(train_seq)\n",
        "        loss = criterion(output, target_seq)\n",
        "        if is_train == True:\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        n_obs += train_seq.size()[0]\n",
        "    elif model_type == 'cond_cnn':\n",
        "      for piano_seq, past_seq, target_seq in dataloader:\n",
        "        piano_seq = piano_seq.to(device)\n",
        "        past_seq = past_seq.to(device)\n",
        "        target_seq = target_seq.to(device)\n",
        "        output, latent = model(past_seq, piano_seq)\n",
        "        loss = criterion(output, target_seq)\n",
        "        if is_train == True:\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        n_obs += piano_seq.size()[0]\n",
        "\n",
        "    # Return average loss for the input sequence\n",
        "    return running_loss / n_obs * 100\n",
        "\n",
        "# Overall training loop\n",
        "def training_loop(model, optimizer, scheduler, criterion, train_dataloader, test_dataloader, model_type = 'cond_cnn', n_epochs = 50):\n",
        "\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    print(scheduler.get_last_lr())\n",
        "    train_epoch_loss = run_epoch(train_dataloader, model, optimizer, criterion, is_train = True, model_type = model_type)\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    test_epoch_loss = run_epoch(test_dataloader, model, optimizer, criterion, is_train = False, model_type = model_type)\n",
        "    test_losses.append(test_epoch_loss)\n",
        "\n",
        "    print('Epoch {}, Train Loss: {}, Test Loss: {}, Time: {}'.format(epoch, train_epoch_loss, test_epoch_loss, datetime.now()))\n",
        "\n",
        "  return train_losses, test_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgBH6-vsK2Je"
      },
      "source": [
        "**Executing New Melody CNN and Conditional CNNs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FSljpObK5HL"
      },
      "outputs": [],
      "source": [
        "melody_train_dataset = MelodyDataset(pianorolls_list, dataset_length = 32 * 8000, seq_length = 32)\n",
        "melody_train_loader = DataLoader(melody_train_dataset, batch_size = 32, drop_last=True)\n",
        "melody_test_dataset = MelodyDataset(pianorolls_list[900:1000], dataset_length = 32 * 2000, seq_length = 32)\n",
        "melody_test_loader = DataLoader(melody_test_dataset, batch_size = 32, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zqJtWhssoBbA",
        "outputId": "54e362ac-b479-4455-c3c7-4669a841058d"
      },
      "outputs": [],
      "source": [
        "n_epochs = 50\n",
        "lr = 0.0005\n",
        "lr_lambda = 0.98\n",
        "\n",
        "melody_cnn = MelodyCNN(latent_size = 128).to(device)\n",
        "optimizer = torch.optim.Adam(melody_cnn.parameters(), lr = lr)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)\n",
        "criterion = nn.MSELoss()\n",
        "train_losses, test_losses = training_loop(melody_cnn, optimizer, scheduler, criterion, melody_train_loader, melody_test_loader, model_type = 'melody_cnn', n_epochs = n_epochs)\n",
        "\n",
        "model_name = 'MelodyCNN_all'\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "torch.save(melody_cnn.state_dict(), save_path)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label = 'Train Loss')\n",
        "plt.plot(test_losses, label = 'Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "for instrument in ['guitar', 'bass', 'strings', 'drums']:\n",
        "  print(instrument)\n",
        "  cond_train_dataset = ConditionalDataset(pianorolls_list, dataset_length = 32 * 8000, seq_length = 32, instrument = instrument)\n",
        "  cond_train_loader = DataLoader(cond_train_dataset, batch_size = 32, drop_last=True)\n",
        "  cond_test_dataset = ConditionalDataset(pianorolls_list[900:1000], dataset_length = 32 * 1000, seq_length = 32, instrument = instrument)\n",
        "  cond_test_loader = DataLoader(cond_test_dataset, batch_size = 32, drop_last=True)\n",
        "\n",
        "  n_epochs = 50\n",
        "  lr = 0.0005\n",
        "  lr_lambda = 0.98\n",
        "\n",
        "  cond_cnn = ConditionalCNN(latent_size = 128).to(device)\n",
        "  optimizer = torch.optim.Adam(cond_cnn.parameters(), lr = lr)\n",
        "  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)\n",
        "  criterion = nn.MSELoss()\n",
        "  train_losses, test_losses = training_loop(cond_cnn, optimizer, scheduler, criterion, cond_train_loader, cond_test_loader, model_type = 'cond_cnn', n_epochs = n_epochs)\n",
        "\n",
        "  model_name = 'CondCNN_all_{}'.format(instrument)\n",
        "  save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "  torch.save(cond_cnn.state_dict(), save_path)\n",
        "  # Plot the losses over epochs\n",
        "  plt.figure()\n",
        "  plt.plot(train_losses, label = 'Train Loss')\n",
        "  plt.plot(test_losses, label = 'Test Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cLTpDYevK5JY",
        "outputId": "6629eeea-5633-42aa-d72a-bc98d4b386f6"
      },
      "outputs": [],
      "source": [
        "for instrument in ['guitar', 'bass', 'strings', 'drums']:\n",
        "  print(instrument)\n",
        "  cond_train_dataset = ConditionalDataset(pianorolls_list[200:1100], dataset_length = 32 * 4000, seq_length = 32, instrument = instrument)\n",
        "  cond_train_loader = DataLoader(cond_train_dataset, batch_size = 32, drop_last=True)\n",
        "  cond_test_dataset = ConditionalDataset(pianorolls_list[0:200], dataset_length = 32 * 1000, seq_length = 32, instrument = instrument)\n",
        "  cond_test_loader = DataLoader(cond_test_dataset, batch_size = 32, drop_last=True)\n",
        "\n",
        "  n_epochs = 30\n",
        "  lr = 0.0005\n",
        "  lr_lambda = 0.99\n",
        "\n",
        "  cond_cnn = ConditionalCNN(latent_size = 64).to(device)\n",
        "  optimizer = torch.optim.Adam(cond_cnn.parameters(), lr = lr)\n",
        "  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)\n",
        "  criterion = nn.MSELoss()\n",
        "  train_losses, test_losses = training_loop(cond_cnn, optimizer, scheduler, criterion, cond_train_loader, cond_test_loader, model_type = 'cond_cnn', n_epochs = n_epochs)\n",
        "\n",
        "  model_name = 'CondCNNv5_{}'.format(instrument)\n",
        "  save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "  torch.save(cond_cnn.state_dict(), save_path)\n",
        "  # Plot the losses over epochs\n",
        "  plt.figure()\n",
        "  plt.plot(train_losses, label = 'Train Loss')\n",
        "  plt.plot(test_losses, label = 'Test Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtECHun3scMu",
        "outputId": "45333cc2-a80e-4c15-b7dc-0f256b3ff004"
      },
      "outputs": [],
      "source": [
        "1+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke0shUlwmI8M"
      },
      "source": [
        "### Executing Old Melody CNN/RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F2tkoa0LhhE"
      },
      "outputs": [],
      "source": [
        "# Code to train all NNs - model_type: cond_cnn, melody_rnn, melody_cnn\n",
        "\n",
        "def train_epoch(dataloader, model, optimizer, criterion, model_type = 'cond_cnn'):\n",
        "  \n",
        "    running_loss = 0\n",
        "    n_obs = 0\n",
        "    for train_seq, target_seq in dataloader:\n",
        "      train_seq = train_seq.to(device)\n",
        "      target_seq = target_seq.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      if model_type == 'melody_rnn':\n",
        "        hidden = model.init_hidden(batch_size = 32)\n",
        "        output, hidden = model(train_seq, hidden)\n",
        "      elif model_type == 'melody_cnn':\n",
        "        output = model(train_seq)\n",
        "      elif model_type == 'cond_cnn':\n",
        "        output, latent = model(train_seq)\n",
        "      loss = criterion(output, target_seq)\n",
        "      loss.backward()\n",
        "      grad_clipping(model, 1)\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      n_obs += train_seq.size()[0]\n",
        "\n",
        "    # Return average loss for the input sequence\n",
        "    return running_loss / n_obs * 100\n",
        "\n",
        "def test_epoch(dataloader, model, optimizer, criterion, model_type = 'cond_cnn'):\n",
        "    \n",
        "    running_loss = 0\n",
        "    n_obs = 0\n",
        "    for train_seq, target_seq in dataloader:\n",
        "      train_seq = train_seq.to(device)\n",
        "      target_seq = target_seq.to(device)\n",
        "      if model_type == 'melody_rnn':\n",
        "        hidden = model.init_hidden(batch_size = 32)\n",
        "        output, hidden = model(train_seq, hidden)\n",
        "      elif model_type == 'melody_cnn':\n",
        "        output = model(train_seq)\n",
        "      elif model_type == 'cond_cnn':\n",
        "        output, latent = model(train_seq)\n",
        "      loss = criterion(output, target_seq)\n",
        "      loss = criterion(output, target_seq)\n",
        "      running_loss += loss.item()\n",
        "      n_obs += train_seq.size()[0]\n",
        "\n",
        "    # Return average loss for the input sequence\n",
        "    return running_loss / n_obs * 100\n",
        "\n",
        "# Overall training loop\n",
        "def training_loop(model, optimizer, scheduler, criterion, train_dataloader, test_dataloader, model_type = 'cond_cnn'):\n",
        "\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    print(scheduler.get_last_lr())\n",
        "    train_epoch_loss = train_epoch(train_dataloader, model, optimizer, criterion, model_type)\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    test_epoch_loss = test_epoch(test_dataloader, model, optimizer, criterion, model_type)\n",
        "    test_losses.append(test_epoch_loss)\n",
        "\n",
        "    print('Epoch {}, Train Loss: {}, Test Loss: {}, Time: {}'.format(epoch, train_epoch_loss, test_epoch_loss, datetime.now()))\n",
        "\n",
        "  return train_losses, test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQiy0AblLXBC"
      },
      "outputs": [],
      "source": [
        "# Code to train all NNs - model_type: cond_cnn, melody_rnn, melody_cnn\n",
        "\n",
        "def train_epoch(dataloader, model, optimizer, criterion, model_type = 'cond_cnn'):\n",
        "  \n",
        "    running_loss = 0\n",
        "    n_obs = 0\n",
        "    for train_seq, target_seq in dataloader:\n",
        "      train_seq = train_seq.to(device)\n",
        "      target_seq = target_seq.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      if model_type == 'melody_rnn':\n",
        "        hidden = model.init_hidden(batch_size = 32)\n",
        "        output, hidden = model(train_seq, hidden)\n",
        "      elif model_type == 'melody_cnn':\n",
        "        output = model(train_seq)\n",
        "      elif model_type == 'cond_cnn':\n",
        "        output, latent = model(train_seq)\n",
        "      loss = criterion(output, target_seq)\n",
        "      loss.backward()\n",
        "      grad_clipping(model, 1)\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      n_obs += train_seq.size()[0]\n",
        "\n",
        "    # Return average loss for the input sequence\n",
        "    return running_loss / n_obs * 100\n",
        "\n",
        "def test_epoch(dataloader, model, optimizer, criterion, model_type = 'cond_cnn'):\n",
        "    \n",
        "    running_loss = 0\n",
        "    n_obs = 0\n",
        "    for train_seq, target_seq in dataloader:\n",
        "      train_seq = train_seq.to(device)\n",
        "      target_seq = target_seq.to(device)\n",
        "      if model_type == 'melody_rnn':\n",
        "        hidden = model.init_hidden(batch_size = 32)\n",
        "        output, hidden = model(train_seq, hidden)\n",
        "      elif model_type == 'melody_cnn':\n",
        "        output = model(train_seq)\n",
        "      elif model_type == 'cond_cnn':\n",
        "        output, latent = model(train_seq)\n",
        "      loss = criterion(output, target_seq)\n",
        "      loss = criterion(output, target_seq)\n",
        "      running_loss += loss.item()\n",
        "      n_obs += train_seq.size()[0]\n",
        "\n",
        "    # Return average loss for the input sequence\n",
        "    return running_loss / n_obs * 100\n",
        "\n",
        "# Overall training loop\n",
        "def training_loop(model, optimizer, scheduler, criterion, train_dataloader, test_dataloader, model_type = 'cond_cnn'):\n",
        "\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    print(scheduler.get_last_lr())\n",
        "    train_epoch_loss = train_epoch(train_dataloader, model, optimizer, criterion, model_type)\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    test_epoch_loss = test_epoch(test_dataloader, model, optimizer, criterion, model_type)\n",
        "    test_losses.append(test_epoch_loss)\n",
        "\n",
        "    print('Epoch {}, Train Loss: {}, Test Loss: {}, Time: {}'.format(epoch, train_epoch_loss, test_epoch_loss, datetime.now()))\n",
        "\n",
        "  return train_losses, test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faBsYzVYSjzd"
      },
      "outputs": [],
      "source": [
        "melody_train_dataset = MelodyDataset(pianorolls_list[0:900], dataset_length = 32 * 8000, seq_length = 32)\n",
        "melody_train_loader = DataLoader(melody_train_dataset, batch_size = 32, drop_last=True)\n",
        "melody_test_dataset = MelodyDataset(pianorolls_list[900:1000], dataset_length = 32 * 2000, seq_length = 32)\n",
        "melody_test_loader = DataLoader(melody_train_dataset, batch_size = 32, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PQZTDYXAijFh",
        "outputId": "1bf7964b-098c-4db1-e7da-593174065734"
      },
      "outputs": [],
      "source": [
        "n_epochs = 30\n",
        "lr = 0.0005\n",
        "lr_lambda = 0.98\n",
        "\n",
        "melody_cnn = MelodyCNN(seq_length = 32).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(melody_cnn.parameters(), lr = lr)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)\n",
        "criterion = nn.MSELoss()\n",
        "train_losses, test_losses = training_loop(melody_cnn, optimizer, scheduler, criterion, melody_train_loader, melody_test_loader, model_type = 'melody_cnn')\n",
        "\n",
        "model_name = 'MelodyCNN'\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "torch.save(melody_cnn.state_dict(), save_path)\n",
        "# Plot the losses over epochs\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label = 'Train Loss')\n",
        "plt.plot(test_losses, label = 'Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ztiyGrVsGlN"
      },
      "outputs": [],
      "source": [
        "model_name = 'MelodyCNN'\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "torch.save(melody_cnn.state_dict(), save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q7N2sq0wSyXQ",
        "outputId": "5c1b50d7-5602-4406-817d-2eadfabfb160"
      },
      "outputs": [],
      "source": [
        "n_epochs = 50\n",
        "lr = 0.0005\n",
        "lr_lambda = 0.98\n",
        "\n",
        "# Create model, optimizer and loss function\n",
        "melody_rnn = MelodyRNN(hidden_size = 64, batch_size = 32, n_layers = 1).to(device)\n",
        "optimizer = torch.optim.Adam(melody_rnn.parameters(), lr = lr)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)\n",
        "criterion = nn.MSELoss()\n",
        "train_losses, test_losses = training_loop(melody_rnn, optimizer, scheduler, criterion, melody_train_loader, melody_test_loader, model_type = 'melody_rnn')\n",
        "\n",
        "model_name = 'MelodyRNN_2'\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "torch.save(melody_rnn.state_dict(), save_path)\n",
        "# Plot the losses over epochs\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label = 'Train Loss')\n",
        "plt.plot(test_losses, label = 'Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "melody_cnn = MelodyCNN(seq_length = 32).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(melody_cnn.parameters(), lr = lr)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)\n",
        "criterion = nn.MSELoss()\n",
        "train_losses, test_losses = training_loop(melody_cnn, optimizer, scheduler, criterion, melody_tra in_loader, melody_test_loader, model_type = 'melody_cnn')\n",
        "\n",
        "model_name = 'MelodyCNN'\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "torch.save(melody_cnn.state_dict(), save_path)\n",
        "# Plot the losses over epochs\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label = 'Train Loss')\n",
        "plt.plot(test_losses, label = 'Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9jFulRQmL-E"
      },
      "source": [
        "**Executing Conditional RNN/CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7fPDP6AOSym6",
        "outputId": "eb46ffa8-161c-4e44-c9da-9b7df66c06ca"
      },
      "outputs": [],
      "source": [
        "for instrument in ['guitar', 'bass', 'strings', 'drums']:\n",
        "  print(instrument)\n",
        "  cond_train_dataset = ConditionalDataset(pianorolls_list[0:900], dataset_length = 32 * 4000, seq_length = 32, instrument = instrument)\n",
        "  cond_train_loader = DataLoader(cond_train_dataset, batch_size = 32, drop_last=True)\n",
        "  cond_test_dataset = ConditionalDataset(pianorolls_list[900:1000], dataset_length = 32 * 1000, seq_length = 32, instrument = instrument)\n",
        "  cond_test_loader = DataLoader(cond_test_dataset, batch_size = 32, drop_last=True)\n",
        "\n",
        "  n_epochs = 50\n",
        "  lr = 0.0005\n",
        "  lr_lambda = 0.98\n",
        "\n",
        "  cond_cnn = ConditionalCNN(latent_size = 64).to(device)\n",
        "  optimizer = torch.optim.Adam(cond_cnn.parameters(), lr = lr)\n",
        "  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)\n",
        "  criterion = nn.MSELoss()\n",
        "  train_losses, test_losses = training_loop(cond_cnn, optimizer, scheduler, criterion, cond_train_loader, cond_test_loader, model_type = 'cond_cnn')\n",
        "\n",
        "  model_name = 'CondCNN_{}'.format(instrument)\n",
        "  save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "  torch.save(cond_cnn.state_dict(), save_path)\n",
        "  # Plot the losses over epochs\n",
        "  plt.figure()\n",
        "  plt.plot(train_losses, label = 'Train Loss')\n",
        "  plt.plot(test_losses, label = 'Test Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB57eIby-Ar7"
      },
      "source": [
        "### Evaluating MelodyCNN/RNN to Generate New Music"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDXil40T8vvD",
        "outputId": "a8ec3cbd-603d-490f-9720-631cbb58a06d"
      },
      "outputs": [],
      "source": [
        "# Note that it's important to put .eval() because if not batchnorm on a batch of size 1 will lead to errors\n",
        "\n",
        "model_version = 4\n",
        "model_name = 'MelodyCNNv{}'.format(model_version - 1)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "melody_model = MelodyCNN(latent_size = 128).to(device)\n",
        "melody_model.load_state_dict(torch.load(save_path))\n",
        "melody_model.eval()\n",
        "\n",
        "model_name = 'CondCNNv{}_guitar'.format(model_version)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "guitar_model = ConditionalCNN(latent_size = 64).to(device)\n",
        "guitar_model.load_state_dict(torch.load(save_path))\n",
        "guitar_model.eval()\n",
        "\n",
        "model_name = 'CondCNNv{}_bass'.format(model_version)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "bass_model = ConditionalCNN(latent_size = 64).to(device)\n",
        "bass_model.load_state_dict(torch.load(save_path))\n",
        "bass_model.eval()\n",
        "\n",
        "model_name = 'CondCNNv{}_strings'.format(model_version)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "strings_model = ConditionalCNN(latent_size = 64).to(device)\n",
        "strings_model.load_state_dict(torch.load(save_path))\n",
        "strings_model.eval()\n",
        "\n",
        "model_name = 'CondCNNv{}_drums'.format(model_version)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "drums_model = ConditionalCNN(latent_size = 64).to(device)\n",
        "drums_model.load_state_dict(torch.load(save_path))\n",
        "drums_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlDoyyWZK0HI"
      },
      "outputs": [],
      "source": [
        "# Generator dataloader and dataset for the actual song generation (gives the full 5 tracks in chunks of length 32)\n",
        "gen_dataset = GenerationDataset(pianorolls_list[0:900], dataset_length = 32 * 10000, seq_length = 32)\n",
        "gen_loader = DataLoader(gen_dataset, batch_size = 1, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoXWsJ6bOA0v"
      },
      "outputs": [],
      "source": [
        "train, test = next(iter(gen_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnUtmK_5C-Od"
      },
      "outputs": [],
      "source": [
        "# Function that takes in previous length-32 5-instrument sequence, and generates a new length-32 5-instrument sequence\n",
        "# Input_sequence: 5 x 32 x 128, models: 5-tuple of the trained NNs\n",
        "# threshold as a % of the maximum output that we will keep the notes there (those under threshold will be moved to 0)\n",
        "# if binarize = True, will set all remaining non-zero to max intensity\n",
        "def generate_new_music(input_sequence, models, threshold = 0.3, binarize = False):\n",
        "\n",
        "  melody_model, guitar_model, bass_model, strings_model, drums_model = models\n",
        "  input_sequence = input_sequence.to(device)\n",
        "\n",
        "  melody_prev = input_sequence[0, :, :].unsqueeze(0)\n",
        "  guitar_prev = input_sequence[1, :, :].unsqueeze(0)\n",
        "  bass_prev = input_sequence[2, :, :].unsqueeze(0)\n",
        "  strings_prev = input_sequence[3, :, :].unsqueeze(0)\n",
        "  drums_prev = input_sequence[4, :, :].unsqueeze(0)\n",
        "\n",
        "  melody_pred, _ = melody_model(melody_prev)\n",
        "  melody_pred = melody_pred / melody_pred.max()\n",
        "  melody_pred[melody_pred < threshold] = 0.0\n",
        "  \n",
        "\n",
        "  guitar_pred, _ = guitar_model(guitar_prev, melody_pred.unsqueeze(0))\n",
        "  guitar_pred = guitar_pred / guitar_pred.max()\n",
        "  guitar_pred[guitar_pred < threshold] = 0.0\n",
        "\n",
        "  bass_pred, _ = bass_model(bass_prev, melody_pred.unsqueeze(0))\n",
        "  bass_pred = bass_pred / bass_pred.max()\n",
        "  bass_pred[bass_pred < threshold] = 0.0\n",
        "\n",
        "  strings_pred, _ = strings_model(strings_prev, melody_pred.unsqueeze(0))\n",
        "  strings_pred = strings_pred / strings_pred.max()\n",
        "  strings_pred[strings_pred < threshold] = 0.0\n",
        "\n",
        "  drums_pred, _ = drums_model(drums_prev, melody_pred.unsqueeze(0))\n",
        "  drums_pred = drums_pred / drums_pred.max()\n",
        "  drums_pred[drums_pred < threshold] = 0.0\n",
        "  creation = torch.cat((melody_pred.unsqueeze(0), guitar_pred.unsqueeze(0), bass_pred.unsqueeze(0), \n",
        "                        strings_pred.unsqueeze(0), drums_pred.unsqueeze(0)), dim = 0)\n",
        "\n",
        "  if binarize == True:\n",
        "    creation[creation > 0] = 1\n",
        "  return creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aivz5Py9ATTu"
      },
      "outputs": [],
      "source": [
        "models = (melody_model, guitar_model, bass_model, strings_model, drums_model)\n",
        "generated_track = torch.zeros((5, 128, 128))\n",
        "generated_track[:, 0:32, :] = train.squeeze()\n",
        "creation = generate_new_music(train.squeeze(), models, threshold = 0.4, binarize = False)\n",
        "generated_track[:, 32:64, :] = creation\n",
        "creation = generate_new_music(creation, models, threshold = 0.4, binarize = False)\n",
        "generated_track[:, 64:96, :] = creation\n",
        "creation = generate_new_music(creation, models, threshold = 0.4, binarize = False)\n",
        "generated_track[:, 96:128, :] = creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "XIxwlsxrMwun",
        "outputId": "6854588c-cf90-4ba5-bda5-f9eb7c7a0c2a"
      },
      "outputs": [],
      "source": [
        "# Only un-normalize here\n",
        "generated_track_out = generated_track * 127\n",
        "# Convert predictions into the multitrack pianoroll\n",
        "piano_track = pypianoroll.StandardTrack(name = 'Piano', program = 0, is_drum = False, pianoroll = generated_track_out[0, :, :].detach().cpu().numpy())\n",
        "guitar_track = pypianoroll.StandardTrack(name = 'Guitar', program = 24, is_drum = False, pianoroll = generated_track_out[1, :, :].detach().cpu().numpy())\n",
        "bass_track = pypianoroll.StandardTrack(name = 'Bass', program = 32, is_drum = False, pianoroll = generated_track_out[2, :, :].cpu().detach().numpy())\n",
        "strings_track = pypianoroll.StandardTrack(name = 'Strings', program = 48, is_drum = False, pianoroll = generated_track_out[3, :, :].cpu().detach().numpy())\n",
        "drums_track = pypianoroll.StandardTrack(name = 'Drums', is_drum = True, pianoroll = generated_track_out[4, :, :].cpu().detach().numpy())\n",
        "generated_multitrack = pypianoroll.Multitrack(name = 'Generated', resolution = 2, tracks = [piano_track, guitar_track, bass_track, strings_track, drums_track])\n",
        "generated_multitrack.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "Q7zcdHHK-tGc",
        "outputId": "8287f253-2ade-44b2-8ec5-8f17df5bed55"
      },
      "outputs": [],
      "source": [
        "generated_pm = pypianoroll.to_pretty_midi(generated_multitrack)\n",
        "generated_midi_audio = generated_pm.fluidsynth()\n",
        "IPython.display.Audio(generated_midi_audio, rate = 44100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSnliZQw8sgs"
      },
      "source": [
        "### Old Way of Generating with MelodyRNN / RNNs (requiring random drawing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPfDL-Gp-Czv",
        "outputId": "1c1521c6-9e03-4b44-e09c-09bc84a3710a"
      },
      "outputs": [],
      "source": [
        "# Generating Melody\n",
        "model_name = 'MelodyRNN_2'\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "model = MelodyRNN(hidden_size = 64, batch_size = 32, n_layers = 1).to(device)\n",
        "model.load_state_dict(torch.load(save_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2NaHGDnAB_i"
      },
      "outputs": [],
      "source": [
        "train, target = next(iter(melody_train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYj7uxOztSNY"
      },
      "outputs": [],
      "source": [
        "prime_seq = train[30, :, 20:30, :].to(device)\n",
        "temperature = 8\n",
        "hidden = model.init_hidden(batch_size = 1)\n",
        "prime_seq = prime_seq.unsqueeze(0)\n",
        "# Build up the hidden state\n",
        "_, hidden = model(prime_seq, hidden)\n",
        "input = prime_seq[:, :, -1:, :]\n",
        "# Make prediction - predicted is 128\n",
        "scores, hidden = model(input, hidden)\n",
        "scores = scores / scores.max()\n",
        "predicted_probs = F.softmax(scores * temperature, dim = 1)\n",
        "predicted_probs[predicted_probs < 0.001] = 0.0\n",
        "predicted_ids = torch.multinomial(predicted_probs, num_samples = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9eS1u_Ut3Hz",
        "outputId": "3bf0dacc-df31-4753-ccc2-f554c47af996"
      },
      "outputs": [],
      "source": [
        "predicted_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqkjydxMt-SD",
        "outputId": "a228453e-0650-40b8-a537-a29792675f2f"
      },
      "outputs": [],
      "source": [
        "predicted_ids[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed3pMBmwHc_4"
      },
      "outputs": [],
      "source": [
        "# Code to evaluate the language model i.e. generate new music\n",
        "# Old code that only generates a fixed number of notes per instrument at any time\n",
        "\n",
        "def evaluate(net, prime_seq, predict_len = 100, temperature = 20):\n",
        "    '''\n",
        "    Arguments:\n",
        "    prime_seq - priming sequence (converted t)\n",
        "    predict_len - number of notes to predict for after prime sequence\n",
        "    '''\n",
        "    hidden = net.init_hidden(batch_size = 1)\n",
        "\n",
        "    # Instantiate new tensor to store predicted sequences\n",
        "    predictions = torch.zeros((5, predict_len + prime_seq.size()[1], 128)).to(device)\n",
        "\n",
        "    # Set the start of the predicted seq to be the prime sequence\n",
        "    predictions[:, 0:prime_seq.size()[1], :] = prime_seq[:, :, :128]\n",
        "\n",
        "    curr_predict_id = prime_seq.size()[1]\n",
        "\n",
        "    # Reshape prime seq \n",
        "    # from n_tracks x seq_length x pitches\n",
        "    # to become batch_size x n_tracks x seq_length x pitches\n",
        "    prime_seq = prime_seq.unsqueeze(0)\n",
        "\n",
        "    # Build up the hidden state\n",
        "    _, hidden = net(prime_seq, hidden)\n",
        "    # Input is last character of prime sequence\n",
        "    input = predictions[:, prime_seq.size()[1] - 1, :]\n",
        "\n",
        "    while curr_predict_id < predictions.size()[1]:\n",
        "      # Forward pass of the trained NN - to get next predicted front\n",
        "      input = input.unsqueeze(0).unsqueeze(2)\n",
        "      predicted, hidden = net(input, hidden)\n",
        "      predicted = predicted / predicted.max()\n",
        "\n",
        "      predicted_probs = F.softmax(predicted * temperature, dim = 1)\n",
        "      predicted_probs[predicted_probs < 0.001] = 0.0\n",
        "      predicted_ids = torch.multinomial(predicted_probs, num_samples = 2)\n",
        "      predicted = torch.zeros((5, 128)).to(device)\n",
        "      predicted[0, predicted_ids[0]] = 1\n",
        "      input = predicted.clone()\n",
        "      predictions[:, curr_predict_id, :] = predicted\n",
        "      \n",
        "      curr_predict_id += 1\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5w4IyNhve-7"
      },
      "outputs": [],
      "source": [
        "# Prime sequence\n",
        "model.to(device)\n",
        "prime_seq = train[16, :, 10:30, 0:128].to(device)\n",
        "predictions = evaluate(model, prime_seq, predict_len = 100, temperature = 6)\n",
        "# Unnormalize\n",
        "predictions = (predictions * 127).type(torch.int8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "xg87KSmoxuJm",
        "outputId": "39d1e1d4-ecc4-4157-eafb-521e12950aee"
      },
      "outputs": [],
      "source": [
        "piano_track.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "O8KoM0uXxJFR",
        "outputId": "d6246b13-4b0f-48ea-d08b-9eff74f546a4"
      },
      "outputs": [],
      "source": [
        "# Convert predictions into the multitrack pianoroll\n",
        "piano_track = pypianoroll.StandardTrack(name = 'Piano', program = 0, is_drum = False, pianoroll = predictions[0, :, :].detach().cpu().numpy())\n",
        "guitar_track = pypianoroll.StandardTrack(name = 'Guitar', program = 24, is_drum = False, pianoroll = predictions[1, :, :].detach().cpu().numpy())\n",
        "bass_track = pypianoroll.StandardTrack(name = 'Bass', program = 32, is_drum = False, pianoroll = predictions[2, :, :].detach().cpu().numpy())\n",
        "strings_track = pypianoroll.StandardTrack(name = 'Strings', program = 48, is_drum = False, pianoroll = predictions[3, :, :].detach().cpu().numpy())\n",
        "drums_track = pypianoroll.StandardTrack(name = 'Drums', is_drum = True, pianoroll = predictions[4, :, :].detach().cpu().numpy())\n",
        "\n",
        "\n",
        "generated_multitrack = pypianoroll.Multitrack(name = 'Generated', resolution = 2, tracks = [piano_track, guitar_track, bass_track, strings_track, drums_track])\n",
        "\n",
        "\n",
        "#resolution=24, tempo=array(shape=(12000,), dtype=float64), downbeat=array(shape=(12000,), dtype=bool)\n",
        "# Plot the generated multitrack\n",
        "generated_multitrack.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "U3UBqL2AxMtJ",
        "outputId": "7aecb36d-d056-4eaf-a36b-510861c6982e"
      },
      "outputs": [],
      "source": [
        "# Convert generated multitrack to pretty midi\n",
        "generated_pm = pypianoroll.to_pretty_midi(generated_multitrack)\n",
        "generated_midi_audio = generated_pm.fluidsynth()\n",
        "IPython.display.Audio(generated_midi_audio, rate = 44100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1VyBGB-DDsf"
      },
      "outputs": [],
      "source": [
        "# Code to evaluate the language model i.e. generate new music\n",
        "# New code that takes in the encoding of how many notes at every time step\n",
        "\n",
        "def evaluateWithNumber(net, prime_seq, predict_len = 100, temperature = 20):\n",
        "    '''\n",
        "    Arguments:\n",
        "    prime_seq - priming sequence (converted t)\n",
        "    predict_len - number of notes to predict for after prime sequence\n",
        "    '''\n",
        "    hidden = model.init_hidden(batch_size = 1)\n",
        "\n",
        "    # Instantiate new tensor to store predicted sequences: n_tracks x seq_length x pitches (128)\n",
        "    predictions = torch.zeros((5, predict_len + prime_seq.size()[1], 128)).to(device)\n",
        "    predictions_n_notes = torch.zeros((5, predict_len + prime_seq.size()[1]))\n",
        "\n",
        "    # Set the start of the predicted seq to be the prime sequence\n",
        "    predictions[:, 0:prime_seq.size()[1], :] = prime_seq[:, :, :128]\n",
        "    curr_predict_id = prime_seq.size()[1]\n",
        "\n",
        "    prime_seq = prime_seq.unsqueeze(0)\n",
        "\n",
        "    # Build up the hidden state\n",
        "    _, hidden = model(prime_seq, hidden)\n",
        "\n",
        "    # prime_seq is 1 x 5 x 50 x 129\n",
        "    input = prime_seq[:, :, -1:, :]\n",
        "    # input is 1 x 5 x 1 x 129\n",
        "\n",
        "    while curr_predict_id < predictions.size()[1]:\n",
        "      \n",
        "      scores, hidden = model(input, hidden)\n",
        "      # predicted is 1 x 645\n",
        "      scores = scores.view(5, 129)\n",
        "      predicted_n_notes = (scores[:, -1] * 2 + 1).type(torch.int8)\n",
        "      scores = scores[:, :-1]\n",
        "\n",
        "      scores = scores / scores.max()\n",
        "      predicted_probs = F.softmax(scores * temperature, dim = 1)\n",
        "      predicted_probs[predicted_probs < 0.001] = 0.0\n",
        "      predicted = torch.zeros_like(scores)\n",
        "\n",
        "      for i in range(5):\n",
        "        instrument_predicted_n_notes = predicted_n_notes[i].item() #  Get number of predicted notes for that instrument\n",
        "        if instrument_predicted_n_notes > 0:\n",
        "          topk, indices = torch.topk(predicted_probs[i, :], instrument_predicted_n_notes * 2) # Twice the number of notes up for consideration (get the top and ignore the rest)\n",
        "          topk[0] = topk[0] / 2\n",
        "          print(topk / topk.sum())\n",
        "          instrument_predicted_indices_ids = torch.multinomial(topk, num_samples = instrument_predicted_n_notes) # Choose from the multinomial \n",
        "          instrument_predicted_ids = torch.gather(indices, 0, instrument_predicted_indices_ids)\n",
        "          predicted[i, instrument_predicted_ids] = 1 # Set the predicted notes ids to 1\n",
        "\n",
        "      # Set next input to just generated prediction\n",
        "      input = predicted.clone()\n",
        "      # Get number of notes for generated prediction\n",
        "      new_predicted_n_notes = (predicted > 0).sum(axis = 1) / 2\n",
        "      predictions_n_notes[:, curr_predict_id] = new_predicted_n_notes\n",
        "      new_predicted_n_notes = new_predicted_n_notes.unsqueeze(1)\n",
        "      input = torch.cat((input, new_predicted_n_notes), dim = 1).unsqueeze(0).unsqueeze(2)\n",
        "      predictions[:, curr_predict_id, :] = predicted\n",
        "\n",
        "      curr_predict_id += 1\n",
        "\n",
        "    return predictions, predictions_n_notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvieKL2NQ3M1",
        "outputId": "36b17477-dbb2-444d-a56a-49252628d874"
      },
      "outputs": [],
      "source": [
        "# Prime sequence\n",
        "model.to(device)\n",
        "prime_seq = train[14, :, 30:80, :].to(device)\n",
        "predictions, predictions_n_notes = evaluateWithNumber(model, prime_seq, predict_len = 100, temperature = 10)\n",
        "# Unnormalize\n",
        "predictions = (predictions * 127).type(torch.int8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gc0Lu52AfRv"
      },
      "outputs": [],
      "source": [
        "# Prime sequence\n",
        "model.to(device)\n",
        "prime_seq = train[29, :, 10:20, 0:128].to(device)\n",
        "predictions = evaluate(model, prime_seq, predict_len = 200, temperature = 10)\n",
        "# Unnormalize\n",
        "predictions = (predictions * 127).type(torch.int8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "zKV5mgLNI6Ou",
        "outputId": "36c9244f-382f-4992-c854-3cf8d7e11be4"
      },
      "outputs": [],
      "source": [
        "# Convert predictions into the multitrack pianoroll\n",
        "piano_track = pypianoroll.StandardTrack(name = 'Piano', program = 0, is_drum = False, pianoroll = predictions[0, :, :].detach().cpu().numpy())\n",
        "guitar_track = pypianoroll.StandardTrack(name = 'Guitar', program = 24, is_drum = False, pianoroll = predictions[1, :, :].detach().cpu().numpy())\n",
        "bass_track = pypianoroll.StandardTrack(name = 'Bass', program = 32, is_drum = False, pianoroll = predictions[2, :, :].detach().cpu().numpy())\n",
        "strings_track = pypianoroll.StandardTrack(name = 'Strings', program = 48, is_drum = False, pianoroll = predictions[3, :, :].detach().cpu().numpy())\n",
        "drums_track = pypianoroll.StandardTrack(name = 'Drums', is_drum = True, pianoroll = predictions[4, :, :].detach().cpu().numpy())\n",
        "\n",
        "\n",
        "generated_multitrack = pypianoroll.Multitrack(name = 'Generated', resolution = 2, tracks = [piano_track, guitar_track, bass_track, strings_track, drums_track])\n",
        "\n",
        "\n",
        "#resolution=24, tempo=array(shape=(12000,), dtype=float64), downbeat=array(shape=(12000,), dtype=bool)\n",
        "# Plot the generated multitrack\n",
        "generated_multitrack.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "ZC2qwLZQI6SV",
        "outputId": "21cada94-dab3-498c-9c47-d7aa1a5da2e7"
      },
      "outputs": [],
      "source": [
        "# Convert generated multitrack to pretty midi\n",
        "generated_pm = pypianoroll.to_pretty_midi(generated_multitrack)\n",
        "generated_midi_audio = generated_pm.fluidsynth()\n",
        "IPython.display.Audio(generated_midi_audio, rate = 44100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ke0shUlwmI8M"
      ],
      "include_colab_link": true,
      "name": "Melody-Conditional Multitrack Generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
