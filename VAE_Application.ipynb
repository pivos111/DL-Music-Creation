{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWReWjdRjQ6P"
      },
      "source": [
        "# Applying VAE to Generate Music in Specific Styles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfcYQM4qjTis"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import pretty_midi\n",
        "import pypianoroll\n",
        "import tables\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "import music21\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "import json\n",
        "import IPython.display\n",
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "import random\n",
        "import itertools\n",
        "root_dir = 'drive/MyDrive/ProjectMusic'\n",
        "data_dir = root_dir + '/Lakh Piano Dataset/LPD-5/lpd_5/lpd_5_cleansed'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMex7xeZoaor",
        "outputId": "5882ca56-9c18-4d04-a0d0-7ece0ae3fde4"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -qU pyfluidsynth pretty_midi\n",
        "!pip install music21\n",
        "!pip install pypianoroll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puztPxIyh3oV",
        "outputId": "edc122ad-5b3e-47e4-ccf6-9ac5e6a8e008"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhR5l1SjNTa5"
      },
      "source": [
        "**Getting MIDI and Song Metadata**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng13lLDrN8Za",
        "outputId": "10e32524-a52c-419f-b028-eca1c7508993"
      },
      "outputs": [],
      "source": [
        "# @title Helper Functions for MIDI Files, Genre File\n",
        "\n",
        "RESULTS_PATH = os.path.join(root_dir, 'Lakh Piano Dataset', 'Metadata')\n",
        "\n",
        "# Utility functions for retrieving paths\n",
        "def msd_id_to_dirs(msd_id):\n",
        "    \"\"\"Given an MSD ID, generate the path prefix.\n",
        "    E.g. TRABCD12345678 -> A/B/C/TRABCD12345678\"\"\"\n",
        "    return os.path.join(msd_id[2], msd_id[3], msd_id[4], msd_id)\n",
        "\n",
        "def msd_id_to_h5(msd_id):\n",
        "    \"\"\"Given an MSD ID, return the path to the corresponding h5\"\"\"\n",
        "    return os.path.join(RESULTS_PATH, 'lmd_matched_h5',\n",
        "                        msd_id_to_dirs(msd_id) + '.h5')\n",
        "\n",
        "# Load the midi npz file from the LMD cleansed folder\n",
        "def get_midi_npz_path(msd_id, midi_md5):\n",
        "    return os.path.join(data_dir,\n",
        "                        msd_id_to_dirs(msd_id), midi_md5 + '.npz')\n",
        "    \n",
        "# Open the cleansed ids - cleansed file ids : msd ids\n",
        "cleansed_ids = pd.read_csv(os.path.join(root_dir, 'Lakh Piano Dataset', 'cleansed_ids.txt'), delimiter = '    ', header = None)\n",
        "lpd_to_msd_ids = {a:b for a, b in zip(cleansed_ids[0], cleansed_ids[1])}\n",
        "msd_to_lpd_ids = {a:b for a, b in zip(cleansed_ids[1], cleansed_ids[0])}\n",
        "\n",
        "# Reading the genre annotations\n",
        "genre_file_dir = os.path.join(root_dir, 'Lakh Piano Dataset', 'Genre', 'msd_tagtraum_cd1.cls')\n",
        "ids = []\n",
        "genres = []\n",
        "with open(genre_file_dir) as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "        if line[0] != '#':\n",
        "          split = line.strip().split(\"\\t\")\n",
        "          if len(split) == 2:\n",
        "            ids.append(split[0])\n",
        "            genres.append(split[1])\n",
        "          elif len(split) == 3:\n",
        "            ids.append(split[0])\n",
        "            ids.append(split[0])\n",
        "            genres.append(split[1])\n",
        "            genres.append(split[2])\n",
        "        line = f.readline()\n",
        "genre_df = pd.DataFrame(data={\"TrackID\": ids, \"Genre\": genres})\n",
        "\n",
        "genre_dict = genre_df.groupby('TrackID')['Genre'].apply(lambda x: x.tolist()).to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Le14xf4NVd"
      },
      "source": [
        "**Objects that we need**\n",
        "\n",
        "- cleansed_ids: dictionary of LPD file name : MSD file name\n",
        "- lmd_metadata: list of dictionaries - each dict has a msd_id field to identify\n",
        "- Get the lmd_file_name (actual path )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOeVzvnwV2ht"
      },
      "outputs": [],
      "source": [
        "# Load the processed metadata\n",
        "with open(os.path.join(root_dir, 'Lakh Piano Dataset', 'processed_metadata.json'), 'r') as outfile:\n",
        "  lmd_metadata = json.load(outfile)\n",
        "\n",
        "# Change this into a dictionary of MSD_ID: metadata\n",
        "lmd_metadata = {e['msd_id']:e for e in lmd_metadata}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU0th4Rt5ZMu",
        "outputId": "fbca76ef-c254-4c5b-f745-1ac9c88ff530"
      },
      "outputs": [],
      "source": [
        "# Get all song MSD IDs in pop rock genre\n",
        "filtered_msd_ids = {k:v['title'] for k, v in lmd_metadata.items() if 'Michael Jackson' in v['artist']}\n",
        "filtered_msd_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkfRkqhiXtqK"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCGGv0lcX9qv",
        "outputId": "a724cff3-def6-4646-c99d-39f689e4bdbe"
      },
      "outputs": [],
      "source": [
        "# Get the most common artists\n",
        "artist_counter = Counter(v['artist'] for v in lmd_metadata.values())\n",
        "artist_counter.most_common(n = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCGQycokYfXR"
      },
      "outputs": [],
      "source": [
        "filtered_msd_ids = ['TRQUIHC128F42ADD0D']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xM2flv96PUg"
      },
      "outputs": [],
      "source": [
        "combined_pianorolls = []\n",
        "i = 0\n",
        "for msd_file_name in filtered_msd_ids:\n",
        "\n",
        "  lpd_file_name = msd_to_lpd_ids[msd_file_name]\n",
        "  # Get the NPZ path\n",
        "  npz_path = get_midi_npz_path(msd_file_name, lpd_file_name)\n",
        "  multitrack = pypianoroll.load(npz_path)\n",
        "  multitrack.set_resolution(2).pad_to_same()\n",
        "\n",
        "  # Piano, Guitar, Bass, Strings, Drums\n",
        "  # Splitting into different parts\n",
        "\n",
        "  parts = {'piano_part': None, 'guitar_part': None, 'bass_part': None, 'strings_part': None, 'drums_part': None}\n",
        "  song_length = None\n",
        "  empty_array = None\n",
        "  has_empty_parts = False\n",
        "  for track in multitrack.tracks:\n",
        "    if track.name == 'Drums':\n",
        "      parts['drums_part'] = track.pianoroll\n",
        "    if track.name == 'Piano':\n",
        "      parts['piano_part'] = track.pianoroll\n",
        "    if track.name == 'Guitar':\n",
        "      parts['guitar_part'] = track.pianoroll\n",
        "    if track.name == 'Bass':\n",
        "      parts['bass_part'] = track.pianoroll\n",
        "    if track.name == 'Strings':\n",
        "      parts['strings_part'] = track.pianoroll\n",
        "    if track.pianoroll.shape[0] > 0:\n",
        "      empty_array = np.zeros_like(track.pianoroll)\n",
        "\n",
        "  for k,v in parts.items():\n",
        "    if v.shape[0] == 0:\n",
        "      parts[k] = empty_array.copy()\n",
        "      has_empty_parts = True\n",
        "\n",
        "  # Stack all together - Piano, Guitar, Bass, Strings, Drums\n",
        "  combined_pianoroll = torch.tensor([parts['piano_part'], parts['guitar_part'], parts['bass_part'], parts['strings_part'], parts['drums_part']])\n",
        "  combined_pianorolls.append(combined_pianoroll)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky_NfEoxZC25",
        "outputId": "39054780-3544-478c-eef4-6f8fff314148"
      },
      "outputs": [],
      "source": [
        "combined_pianoroll.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TeycNi6YyCm"
      },
      "outputs": [],
      "source": [
        "# Saving\n",
        "pianoroll_lengths = [e.size()[1] for e in combined_pianorolls]\n",
        "combined_pianorolls = torch.hstack(combined_pianorolls)\n",
        "\n",
        "torch.save(combined_pianorolls, os.path.join(root_dir, 'Lakh Piano Dataset', 'metal_1000_pianorolls.pt'))\n",
        "pianoroll_lengths = torch.tensor(pianoroll_lengths)\n",
        "torch.save(pianoroll_lengths, os.path.join(root_dir, 'Lakh Piano Dataset', 'metal_1000_pianorolls_lengths.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wd2LnhQ-z7w"
      },
      "outputs": [],
      "source": [
        "# Loading the entire dataset\n",
        "combined_pianorolls = torch.load(os.path.join(root_dir, 'Lakh Piano Dataset', 'rock_1000_pianorolls_res2.pt'))\n",
        "pianoroll_lengths = torch.load(os.path.join(root_dir, 'Lakh Piano Dataset', 'rock_1000_pianorolls_res2_lengths.pt'))\n",
        "pianoroll_lengths = pianoroll_lengths.numpy()\n",
        "pianoroll_cum_lengths = pianoroll_lengths.cumsum()\n",
        "\n",
        "# Normalize\n",
        "combined_pianorolls = combined_pianorolls / 127.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n49U3aXeavYV"
      },
      "outputs": [],
      "source": [
        "# Remake the list of pianorolls - ensuring all songs are multiple of 32\n",
        "pianorolls_list = []\n",
        "pianorolls_list.append(combined_pianorolls[:, :(pianoroll_cum_lengths[0] - pianoroll_cum_lengths[0] % 32), :])\n",
        "for i in range(len(pianoroll_cum_lengths) - 1):\n",
        "  length = pianoroll_cum_lengths[i+1] - pianoroll_cum_lengths[i]\n",
        "  # Get the nearest multiple of 32\n",
        "  length_multiple = length - (length % 32)\n",
        "  pianoroll = combined_pianorolls[:, pianoroll_cum_lengths[i]:(pianoroll_cum_lengths[i] + length_multiple), :]\n",
        "  pianorolls_list.append(pianoroll)\n",
        "\n",
        "# Combine the pianorolls again\n",
        "combined_pianorolls = torch.hstack(pianorolls_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r82cg5RvbJSy"
      },
      "source": [
        "**Creating Music Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3lZ7cHoGMcW"
      },
      "outputs": [],
      "source": [
        "# Creating dataset and dataloader\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dY-FejpcdF0q"
      },
      "outputs": [],
      "source": [
        "# @title Old Datasets used for Training\n",
        "\n",
        "# Dataset which only returns sequences which are multiples of 32\n",
        "class CombinedDataset(Dataset):\n",
        "  def __init__(self, pianorolls, instrument_id):\n",
        "    self.data = pianorolls\n",
        "    self.length = int(pianorolls.size(1) / 32)\n",
        "    self.instrument_id = instrument_id\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sequence = self.data[self.instrument_id, (index * 32):((index+1) * 32), :]\n",
        "    return sequence\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "# Melody-conditional dataset NEW - returns BOTH the previous harmony, and current melody, and current harmony\n",
        "# only outputs samples with all tracks non-empty\n",
        "class ConditionalDataset(Dataset):\n",
        "  def __init__(self, list_of_sequences, dataset_length = 32 * 1000, seq_length = 50, instrument = 'guitar'):\n",
        "\n",
        "    self.data = list_of_sequences\n",
        "    self.n_songs = len(list_of_sequences)\n",
        "    self.seq_length = seq_length\n",
        "    self.length = dataset_length\n",
        "    self.instrument = instrument\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Choose a random song id\n",
        "    valid_sequence = False\n",
        "\n",
        "    while valid_sequence == False:\n",
        "      song_id = random.randint(0, self.n_songs - 1)\n",
        "      song_length = self.data[song_id].size()[1]\n",
        "\n",
        "      # Choose a random start window\n",
        "      start_time = random.randint(0, song_length - self.seq_length * 2 - 2)\n",
        "      start_time = start_time - (start_time % 32)\n",
        "\n",
        "      # train_sequence: 1 (piano) x seq_length x 128\n",
        "      piano_sequence = self.data[song_id][0, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "\n",
        "      if self.instrument == 'guitar':\n",
        "        past_sequence = self.data[song_id][1, start_time:(start_time + self.seq_length), :]\n",
        "        target_sequence = self.data[song_id][1, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "      elif self.instrument == 'bass':\n",
        "        past_sequence = self.data[song_id][2, start_time:(start_time + self.seq_length), :]\n",
        "        target_sequence = self.data[song_id][2, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "      elif self.instrument == 'strings':\n",
        "        past_sequence = self.data[song_id][3, start_time:(start_time + self.seq_length), :]\n",
        "        target_sequence = self.data[song_id][3, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "      elif self.instrument == 'drums':\n",
        "        past_sequence = self.data[song_id][4, start_time:(start_time + self.seq_length), :]\n",
        "        target_sequence = self.data[song_id][4, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "      else:\n",
        "        past_sequence = None\n",
        "        target_sequence = None\n",
        "\n",
        "      if piano_sequence.sum() != 0 and past_sequence.sum() != 0 and target_sequence.sum() != 0:\n",
        "        valid_sequence = True\n",
        "      else:\n",
        "        if random.random() < 0.1:\n",
        "          valid_sequence = True\n",
        "          \n",
        "    return piano_sequence, past_sequence, target_sequence\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "# Melody prediction dataset - predict the next melody given the current melody\n",
        "class MelodyDataset(Dataset):\n",
        "  def __init__(self, list_of_sequences, dataset_length = 32 * 10000, seq_length = 50):\n",
        "\n",
        "    # Don't normalize anymore since it was done earlier\n",
        "    self.data = list_of_sequences\n",
        "    self.n_songs = len(list_of_sequences)\n",
        "    self.seq_length = seq_length\n",
        "    self.length = dataset_length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Choose a random song id\n",
        "    song_id = random.randint(0, self.n_songs - 1)\n",
        "    song_length = self.data[song_id].size()[1]\n",
        "    # Choose a random start window\n",
        "    start_time = random.randint(0, song_length - self.seq_length * 2 - 2)\n",
        "    start_time = start_time - (start_time % 32)\n",
        "    # train_sequence: 1 (piano) x seq_length x 128\n",
        "    train_sequence = self.data[song_id][0, start_time:(start_time + self.seq_length), :]\n",
        "    # target_sequence: 1 (piano) x seq_length x 128\n",
        "    target_sequence = self.data[song_id][0, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "    return train_sequence, target_sequence\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePY0SzQcgerO"
      },
      "source": [
        "### VAE Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4hrgejAznL95"
      },
      "outputs": [],
      "source": [
        "# @title VAE Helper Functions\n",
        "\n",
        "def kl_q_p(zs, phi):\n",
        "    \"\"\"Given [b,n,k] samples of z drawn from q, compute estimate of KL(q||p).\n",
        "    phi must be size [b,k+1]\n",
        "\n",
        "    This uses mu_p = 0 and sigma_p = 1, which simplifies the log(p(zs)) term to\n",
        "    just -1/2*(zs**2)\n",
        "    \"\"\"\n",
        "    b, n, k = zs.size()\n",
        "    mu_q, log_sig_q = phi[:,:-1], phi[:,-1]\n",
        "    log_p = -0.5*(zs**2)\n",
        "    log_q = -0.5*(zs - mu_q.view(b,1,k))**2 / log_sig_q.exp().view(b,1,1)**2 - log_sig_q.view(b,1,-1)\n",
        "    # Size of log_q and log_p is [b,n,k]. Sum along [k] but mean along [b,n]\n",
        "    return (log_q - log_p).sum(dim=2).mean(dim=(0,1))\n",
        "\n",
        "def log_p_x(x, mu_xs, sig_x):\n",
        "    \"\"\"Given [batch, ...] input x and [batch, n, ...] reconstructions, compute\n",
        "    pixel-wise log Gaussian probability\n",
        "\n",
        "    Sum over pixel dimensions, but mean over batch and samples.\n",
        "    \"\"\"\n",
        "    b, n = mu_xs.size()[:2]\n",
        "    # Flatten out pixels and add a singleton dimension [1] so that x will be\n",
        "    # implicitly expanded when combined with mu_xs\n",
        "    x = x.reshape(b, 1, -1)\n",
        "    _, _, p = x.size()\n",
        "    squared_error = (x - mu_xs.view(b, n, -1))**2 / (2*sig_x**2)\n",
        "\n",
        "    # Size of squared_error is [b,n,p]. log prob is by definition sum over [p].\n",
        "    # Expected value requires mean over [n]. Handling different size batches\n",
        "    # requires mean over [b].\n",
        "    return -(squared_error + torch.log(sig_x)).sum(dim=2).mean(dim=(0,1))\n",
        "\n",
        "def rsample(phi, n_samples):\n",
        "    \"\"\"Sample z ~ q(z;phi)\n",
        "    Ouput z is size [b,n_samples,K] given phi with shape [b,K+1]. The first K\n",
        "    entries of each row of phi are the mean of q, and phi[:,-1] is the log\n",
        "    standard deviation\n",
        "    \"\"\"\n",
        "    b, kplus1 = phi.size()\n",
        "    k = kplus1-1\n",
        "    mu, sig = phi[:, :-1], phi[:,-1].exp()\n",
        "    eps = torch.randn(b, n_samples, k, device=phi.device)\n",
        "    return eps*sig.view(b,1,1) + mu.view(b,1,k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7blEIJAEggIF"
      },
      "outputs": [],
      "source": [
        "# @title ConvVAE Architecture\n",
        "\n",
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self, K, num_filters=32, filter_size=5):\n",
        "        super(ConvVAE, self).__init__()\n",
        "\n",
        "        # Define the recognition model (encoder or q) part\n",
        "        # Input size: num_channels (1) x seq_length (32) x n_pitches (128)\n",
        "        self.q_conv_1 = nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = (4, 4), stride = (4, 4))\n",
        "        self.q_conv_2 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = (4, 4), stride = (4, 4))\n",
        "        self.q_conv_3 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = (2, 8), stride = (2, 8))\n",
        "        self.q_fc_phi = nn.Linear(256, K+1)\n",
        "\n",
        "        # Define the generative model (decoder or p) part\n",
        "        self.p_fc_upsample = nn.Linear(K, 256)\n",
        "        self.p_deconv_1 = nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = (2, 8), stride = (2, 8))\n",
        "        self.p_deconv_2 = nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = (4, 4), stride = (4, 4))\n",
        "        self.p_deconv_3 = nn.ConvTranspose2d(in_channels = 64, out_channels = 1, kernel_size = (4, 4), stride = (4, 4))\n",
        "\n",
        "        # Define a special extra parameter to learn scalar sig_x for all pixels\n",
        "        self.log_sig_x = nn.Parameter(torch.zeros(()))\n",
        "    \n",
        "    def infer(self, x):\n",
        "        \"\"\"Map (batch of) x to (batch of) phi which can then be passed to\n",
        "        rsample to get z\n",
        "        \"\"\"\n",
        "        x = x.unsqueeze(1)\n",
        "        s = F.relu(self.q_conv_1(x))\n",
        "        s = F.relu(self.q_conv_2(s))\n",
        "        s = F.relu(self.q_conv_3(s))\n",
        "        # Flatten s\n",
        "        flat_s = s.view(s.size()[0], -1)\n",
        "        phi = self.q_fc_phi(flat_s)\n",
        "        return phi\n",
        "\n",
        "    def generate(self, zs):\n",
        "        \"\"\"Map [b,n,k] sized samples of z to [b,n,p] sized images\n",
        "        \"\"\"\n",
        "        # Note that for the purposes of passing through the generator, we need\n",
        "        # to reshape zs to be size [b*n,k]\n",
        "        b, n, k = zs.size()\n",
        "        s = zs.view(b*n, -1)\n",
        "        # Unflatten\n",
        "        s = F.relu(self.p_fc_upsample(s)).unsqueeze(2).unsqueeze(3)\n",
        "        s = F.relu(self.p_deconv_1(s))\n",
        "        s = F.relu(self.p_deconv_2(s))\n",
        "        s = self.p_deconv_3(s)\n",
        "        mu_xs = s.view(b, n, -1)\n",
        "        return mu_xs\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # VAE.forward() is not used for training, but we'll treat it like a\n",
        "        # classic autoencoder by taking a single sample of z ~ q\n",
        "        phi = self.infer(x)\n",
        "        zs = rsample(phi, 1)\n",
        "        return self.generate(zs).view(x.size())\n",
        "\n",
        "    def elbo(self, x, n=1):\n",
        "        \"\"\"Run input end to end through the VAE and compute the ELBO using n\n",
        "        samples of z\n",
        "        \"\"\"\n",
        "        phi = self.infer(x)\n",
        "        zs = rsample(phi, n)\n",
        "        mu_xs = self.generate(zs)\n",
        "        return log_p_x(x, mu_xs, self.log_sig_x.exp()) - kl_q_p(zs, phi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8d-t-KUutu9Q"
      },
      "outputs": [],
      "source": [
        "# @title Conditional and Melody NN Architectures\n",
        "\n",
        "# Conditional NN - uses current melody and previous harmony's LATENT vectors to predict next harmony's LATENT vectors\n",
        "class ConditionalNN(nn.Module):\n",
        "    def __init__(self, K):\n",
        "        super(ConditionalNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(2*K, 128)\n",
        "        self.fc2 = nn.Linear(128, K)\n",
        "\n",
        "    def forward(self, prev_harmony, melody):\n",
        "\n",
        "      x = torch.cat((prev_harmony, melody), axis = 1)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      out = self.fc2(x)\n",
        "      return out\n",
        "\n",
        "# Melody NN - uses previous melody's LATENT vectors to predict next melody's LATENT VECTORS\n",
        "class MelodyNN(nn.Module):\n",
        "    def __init__(self, K):\n",
        "        super(MelodyNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(K, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, K)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = self.dropout(x)\n",
        "      x = F.relu(self.fc2(x))\n",
        "      out = self.fc3(x)\n",
        "      return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mww8pOlPKwDs"
      },
      "source": [
        "### Using Trained VAEs and NNs to Generate Music"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THJM88UQB0xN",
        "outputId": "d280ae72-a54a-4157-df47-0997c0a50b72"
      },
      "outputs": [],
      "source": [
        "# @title Load trained VAEs and NNs (Run me)\n",
        "\n",
        "# Specify dimensionality of VAEs you want (K = 8, 16, 32, 64)\n",
        "K = 16\n",
        "\n",
        "# Load VAEs\n",
        "model_name = 'VAE_piano_{}'.format(K)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', 'VAE', 'Large Data', model_name)\n",
        "piano_vae = ConvVAE(K=K).to(device)\n",
        "piano_vae.load_state_dict(torch.load(save_path))\n",
        "piano_vae.eval()\n",
        "\n",
        "model_name = 'VAE_guitar_{}'.format(K)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', 'VAE', 'Large Data', model_name)\n",
        "guitar_vae = ConvVAE(K=K).to(device)\n",
        "guitar_vae.load_state_dict(torch.load(save_path))\n",
        "guitar_vae.eval()\n",
        "\n",
        "model_name = 'VAE_bass_{}'.format(K)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', 'VAE', 'Large Data', model_name)\n",
        "bass_vae = ConvVAE(K=K).to(device)\n",
        "bass_vae.load_state_dict(torch.load(save_path))\n",
        "bass_vae.eval()\n",
        "\n",
        "model_name = 'VAE_strings_{}'.format(K)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', 'VAE', 'Large Data', model_name)\n",
        "strings_vae = ConvVAE(K=K).to(device)\n",
        "strings_vae.load_state_dict(torch.load(save_path))\n",
        "strings_vae.eval()\n",
        "\n",
        "model_name = 'VAE_drums_{}'.format(K)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', 'VAE', 'Large Data', model_name)\n",
        "drums_vae = ConvVAE(K=K).to(device)\n",
        "drums_vae.load_state_dict(torch.load(save_path))\n",
        "drums_vae.eval()\n",
        "\n",
        "# Load Melody NN\n",
        "model_name = 'VAE_NN_piano_{}'.format(K)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', 'VAE', 'Large Data', model_name)\n",
        "melody_nn = MelodyNN(K = K).to(device)\n",
        "melody_nn.load_state_dict(torch.load(save_path))\n",
        "melody_nn.eval()\n",
        "\n",
        "# Load Conditional NNs\n",
        "model_name = 'VAE_NN_guitar_{}'.format(K)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', 'VAE', 'Large Data', model_name)\n",
        "guitar_nn = ConditionalNN(K = K).to(device)\n",
        "guitar_nn.load_state_dict(torch.load(save_path))\n",
        "guitar_nn.eval()\n",
        "\n",
        "model_name = 'VAE_NN_bass_{}'.format(K)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', 'VAE', 'Large Data', model_name)\n",
        "bass_nn = ConditionalNN(K = K).to(device)\n",
        "bass_nn.load_state_dict(torch.load(save_path))\n",
        "guitar_nn.eval()\n",
        "\n",
        "model_name = 'VAE_NN_strings_{}'.format(K)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', 'VAE', 'Large Data', model_name)\n",
        "strings_nn = ConditionalNN(K = K).to(device)\n",
        "strings_nn.load_state_dict(torch.load(save_path))\n",
        "strings_nn.eval()\n",
        "\n",
        "model_name = 'VAE_NN_drums_{}'.format(K)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', 'VAE', 'Large Data', model_name)\n",
        "drums_nn = ConditionalNN(K = K).to(device)\n",
        "drums_nn.load_state_dict(torch.load(save_path))\n",
        "drums_nn.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7pov-wPeDsoC"
      },
      "outputs": [],
      "source": [
        "# @title Generation Dataset\n",
        "\n",
        "# all fields cannot be blank\n",
        "class GenerationDataset(Dataset):\n",
        "  def __init__(self, list_of_sequences, dataset_length = 32 * 10000, seq_length = 50):\n",
        "\n",
        "    # Don't normalize anymore since it was done earlier\n",
        "    self.data = list_of_sequences\n",
        "    self.n_songs = len(list_of_sequences)\n",
        "    self.seq_length = seq_length\n",
        "    self.length = dataset_length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    valid_sequence = False\n",
        "    while valid_sequence == False:\n",
        "      # Choose a random song id\n",
        "      song_id = random.randint(0, self.n_songs - 1)\n",
        "      song_length = self.data[song_id].size()[1]\n",
        "      # Choose a random start window\n",
        "      start_time = random.randint(0, song_length - self.seq_length * 2 - 2)\n",
        "      start_time = start_time - (start_time % 32)\n",
        "      # Check that every track is not empty\n",
        "      piano_sequence = self.data[song_id][0, start_time:(start_time + self.seq_length), :]\n",
        "      guitar_sequence = self.data[song_id][1, start_time:(start_time + self.seq_length), :]\n",
        "      bass_sequence = self.data[song_id][2, start_time:(start_time + self.seq_length), :]\n",
        "      strings_sequence = self.data[song_id][3, start_time:(start_time + self.seq_length), :]\n",
        "      drums_sequence = self.data[song_id][4, start_time:(start_time + self.seq_length), :]\n",
        "\n",
        "      if piano_sequence.sum() != 0 and guitar_sequence.sum() != 0 and bass_sequence.sum() != 0 \\\n",
        "      and strings_sequence.sum() != 0 and drums_sequence.sum() != 0:\n",
        "        valid_sequence = True\n",
        "      else:\n",
        "        if random.random() < 0.1:\n",
        "          valid_sequence = True\n",
        "\n",
        "    train_sequence = self.data[song_id][:, start_time:(start_time + self.seq_length), :]\n",
        "    target_sequence = self.data[song_id][:, (start_time + self.seq_length):(start_time + self.seq_length * 2), :]\n",
        "    return train_sequence, target_sequence\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vpeQtxkyaHi1"
      },
      "outputs": [],
      "source": [
        "# @title ExternalSongGenerationDataset\n",
        "\n",
        "# all fields cannot be blank\n",
        "class ExternalSongGenerationDataset(Dataset):\n",
        "  def __init__(self, song_pianoroll, seq_length = 50):\n",
        "\n",
        "    # Don't normalize anymore since it was done earlier\n",
        "    self.data = song_pianoroll\n",
        "    self.seq_length = seq_length\n",
        "    self.length = int(song_pianoroll.size(1) / seq_length)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    piano_sequence = self.data[0, (index*32):((index+1)*32), :]\n",
        "    guitar_sequence = self.data[1, (index*32):((index+1)*32), :]\n",
        "    bass_sequence = self.data[2, (index*32):((index+1)*32), :]\n",
        "    strings_sequence = self.data[3, (index*32):((index+1)*32), :]\n",
        "    drums_sequence = self.data[4, (index*32):((index+1)*32), :]\n",
        "\n",
        "    return piano_sequence, guitar_sequence, bass_sequence, strings_sequence, drums_sequence\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYiecui4ZYoI"
      },
      "outputs": [],
      "source": [
        "song_pianoroll = combined_pianorolls[0]\n",
        "song_pianoroll = song_pianoroll / 127.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl2GZYw7ZugP"
      },
      "outputs": [],
      "source": [
        "# From combined_dataset, parse out all the parts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBOUJUXibhVy"
      },
      "outputs": [],
      "source": [
        "piano_sequence, guitar_sequence, bass_sequence, strings_sequence, drums_sequence = next(iter(song_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bAysmdtNbBJX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @title Generate Latent Samples function\n",
        "def generate_latent_samples(song_loader):\n",
        "\n",
        "  # Get list of the song's latent vectors\n",
        "  song_latent_list = {'piano': [], 'guitar': [], 'bass': [], 'strings': [], 'drums': []}\n",
        "  piano_empty_added = False\n",
        "  guitar_empty_added = False\n",
        "  bass_empty_added  = False\n",
        "  strings_empty_added = False\n",
        "  drums_empty_added = False\n",
        "  for piano_sequence, guitar_sequence, bass_sequence, strings_sequence, drums_sequence in song_loader:\n",
        "    if piano_sequence.sum() != 0:\n",
        "      piano_latent = piano_vae.infer(piano_sequence.to(device))[:, :-1]\n",
        "      song_latent_list['piano'].append(piano_latent)\n",
        "    elif piano_empty_added == False:\n",
        "      piano_latent = piano_vae.infer(piano_sequence.to(device))[:, :-1]\n",
        "      song_latent_list['piano'].append(piano_latent)\n",
        "      piano_empty_added = True\n",
        "\n",
        "    if guitar_sequence.sum() != 0:\n",
        "      guitar_latent = guitar_vae.infer(guitar_sequence.to(device))[:, :-1]\n",
        "      song_latent_list['guitar'].append(guitar_latent)\n",
        "    elif guitar_empty_added == False:\n",
        "      guitar_latent = guitar_vae.infer(guitar_sequence.to(device))[:, :-1]\n",
        "      song_latent_list['guitar'].append(guitar_latent)\n",
        "      guitar_empty_added = True\n",
        "      \n",
        "    if bass_sequence.sum() != 0:\n",
        "      bass_latent = bass_vae.infer(bass_sequence.to(device))[:, :-1]\n",
        "      song_latent_list['bass'].append(bass_latent)\n",
        "    elif bass_empty_added == False:\n",
        "      bass_latent = bass_vae.infer(bass_sequence.to(device))[:, :-1]\n",
        "      song_latent_list['bass'].append(bass_latent)\n",
        "      bass_empty_added = True\n",
        "\n",
        "    if strings_sequence.sum() != 0:\n",
        "      strings_latent = strings_vae.infer(strings_sequence.to(device))[:, :-1]\n",
        "      song_latent_list['strings'].append(strings_latent)\n",
        "    elif strings_empty_added == False:\n",
        "      strings_latent = strings_vae.infer(strings_sequence.to(device))[:, :-1]\n",
        "      song_latent_list['strings'].append(strings_latent)\n",
        "      strings_empty_added = True\n",
        "\n",
        "    if drums_sequence.sum() != 0:\n",
        "      drums_latent = drums_vae.infer(drums_sequence.to(device))[:, :-1]\n",
        "      song_latent_list['drums'].append(drums_latent)\n",
        "    elif drums_empty_added == False:\n",
        "      drums_latent = drums_vae.infer(drums_sequence.to(device))[:, :-1]\n",
        "      song_latent_list['drums'].append(drums_latent)\n",
        "      drums_empty_added = True\n",
        "\n",
        "  return song_latent_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_ievpwYEYzv"
      },
      "outputs": [],
      "source": [
        "# Function that takes in previous length-32 5-instrument sequence, and generates a new length-32 5-instrument sequence\n",
        "# Input_sequence: 5 x 32 x 128, vae_models: 5-tuple of the trained VAEs, nn_models: 5-tuple of trained NNs \n",
        "# threshold as a % of the maximum output that we will keep the notes there (those under threshold will be moved to 0)\n",
        "# if binarize = True, will set all remaining non-zero to max intensity\n",
        "def generate_music_vae(sample, vae_models, nn_models, noise_sd = 0, threshold = 0.3, binarize = True, latent_samples = None, latent_sample_factor = 0.5):\n",
        "\n",
        "  piano_vae, guitar_vae, bass_vae, strings_vae, drums_vae = vae_models\n",
        "  melody_nn, guitar_nn, bass_nn, strings_nn, drums_nn = nn_models\n",
        "\n",
        "  piano, guitar, bass, strings, drums = sample[0, :, :], sample[1, :, :], sample[2, :, :], sample[3, :, :], sample[4, :, :]\n",
        "\n",
        "  # Convert all part from image space to latent space - {instr}_latent: batch_size x K\n",
        "  piano_latent = piano_vae.infer(piano.unsqueeze(0).to(device))[:, :-1]\n",
        "  guitar_latent = guitar_vae.infer(guitar.unsqueeze(0).to(device))[:, :-1]\n",
        "  bass_latent = bass_vae.infer(bass.unsqueeze(0).to(device))[:, :-1]\n",
        "  strings_latent = strings_vae.infer(strings.unsqueeze(0).to(device))[:, :-1]\n",
        "  drums_latent = drums_vae.infer(drums.unsqueeze(0).to(device))[:, :-1]\n",
        "\n",
        "  if latent_samples: # Choose a random \n",
        "    piano_latent_sample = random.choice(latent_samples['piano'])\n",
        "    guitar_latent_sample = random.choice(latent_samples['guitar'])\n",
        "    bass_latent_sample = random.choice(latent_samples['bass'])\n",
        "    strings_latent_sample = random.choice(latent_samples['strings'])\n",
        "    drums_latent_sample = random.choice(latent_samples['drums'])\n",
        "\n",
        "    # Interpolate between the past latent and sample latent\n",
        "    piano_latent = latent_sample_factor * piano_latent_sample + (1-latent_sample_factor) * piano_latent\n",
        "    piano_latent = latent_sample_factor * guitar_latent_sample + (1-latent_sample_factor) * guitar_latent\n",
        "    piano_latent = latent_sample_factor * bass_latent_sample + (1-latent_sample_factor) * bass_latent\n",
        "    piano_latent = latent_sample_factor * strings_latent_sample + (1-latent_sample_factor) * strings_latent\n",
        "    piano_latent = latent_sample_factor * drums_latent_sample + (1-latent_sample_factor) * drums_latent\n",
        "\n",
        "\n",
        "  # Use melody NN to convert past piano latent to next piano latent - piano_next_latent: batch_size x K\n",
        "  piano_next_latent = melody_nn(piano_latent)\n",
        "  # Add some noise\n",
        "  random_noise = torch.randn_like(piano_next_latent) * noise_sd\n",
        "  piano_next_latent = piano_next_latent + random_noise\n",
        "\n",
        "  # Use conditional NNs to convert piano latent to instrument latent, and add noise - {istr})_next_latent: batch_size x K\n",
        "  guitar_next_latent = guitar_nn(guitar_latent, piano_next_latent) + torch.randn_like(piano_next_latent) * noise_sd\n",
        "  bass_next_latent = bass_nn(bass_latent, piano_next_latent) + torch.randn_like(piano_next_latent) * noise_sd\n",
        "  strings_next_latent = strings_nn(strings_latent, piano_next_latent) + torch.randn_like(piano_next_latent) * noise_sd\n",
        "  drums_next_latent = drums_nn(drums_latent, piano_next_latent) + torch.randn_like(piano_next_latent) * noise_sd\n",
        "\n",
        "  # Generate new samples given new latent\n",
        "  piano_next = piano_vae.generate(piano_next_latent.unsqueeze(0)).view(1, 32, 128)\n",
        "  guitar_next = guitar_vae.generate(guitar_next_latent.unsqueeze(0)).view(1, 32, 128)\n",
        "  bass_next = bass_vae.generate(bass_next_latent.unsqueeze(0)).view(1, 32, 128)\n",
        "  strings_next = strings_vae.generate(strings_next_latent.unsqueeze(0)).view(1, 32, 128)\n",
        "  drums_next = drums_vae.generate(drums_next_latent.unsqueeze(0)).view(1, 32, 128)\n",
        "\n",
        "  creation = torch.cat((piano_next, guitar_next, bass_next, strings_next, drums_next), dim = 0)\n",
        "  creation[creation < threshold] = 0\n",
        "\n",
        "  if binarize == True:\n",
        "    creation[creation > 0] = 0.8\n",
        "\n",
        "    # Quieten the strings\n",
        "    creation[3, :, :] = creation[3, :, :] * 0.75\n",
        "\n",
        "  return creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn0Jq2MqIkAf"
      },
      "outputs": [],
      "source": [
        "vae_models = (piano_vae, guitar_vae, bass_vae, strings_vae, drums_vae)\n",
        "nn_models = (melody_nn, guitar_nn, bass_nn, strings_nn, drums_nn)\n",
        "sample = song_pianoroll[:, 320:352]\n",
        "\n",
        "song_dataset = ExternalSongGenerationDataset(song_pianoroll, seq_length = 32)\n",
        "song_loader = DataLoader(song_dataset, batch_size = 1, shuffle = False)\n",
        "song_latent_list = generate_latent_samples(song_loader)\n",
        "\n",
        "# Code to essentially recurrently generate music\n",
        "prediction_steps = 8\n",
        "generated_track = torch.zeros((5, 32 * (prediction_steps + 1), 128)).to(device)\n",
        "generated_track[:, :32, :] = sample\n",
        "\n",
        "for i in range(1, prediction_steps + 1):\n",
        "  sample = generate_music_vae(sample, vae_models, nn_models, noise_sd = 1, threshold = 0.2, binarize = True, latent_samples = song_latent_list, latent_sample_factor = 0.5)\n",
        "  generated_track[:,32*i:32*(i+1) , :] = sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "ZoUlSnInJZFO",
        "outputId": "bc1e3d58-d4cd-4673-c355-7e8fe0657e06"
      },
      "outputs": [],
      "source": [
        "# Only un-normalize here\n",
        "generated_track_out = generated_track * 127\n",
        "# Convert predictions into the multitrack pianoroll\n",
        "piano_track = pypianoroll.StandardTrack(name = 'Piano', program = 0, is_drum = False, pianoroll = generated_track_out[0, :, :].detach().cpu().numpy())\n",
        "guitar_track = pypianoroll.StandardTrack(name = 'Guitar', program = 24, is_drum = False, pianoroll = generated_track_out[1, :, :].detach().cpu().numpy())\n",
        "bass_track = pypianoroll.StandardTrack(name = 'Bass', program = 32, is_drum = False, pianoroll = generated_track_out[2, :, :].cpu().detach().numpy())\n",
        "strings_track = pypianoroll.StandardTrack(name = 'Strings', program = 48, is_drum = False, pianoroll = generated_track_out[3, :, :].cpu().detach().numpy())\n",
        "drums_track = pypianoroll.StandardTrack(name = 'Drums', is_drum = True, pianoroll = generated_track_out[4, :, :].cpu().detach().numpy())\n",
        "generated_multitrack = pypianoroll.Multitrack(name = 'Generated', resolution = 2, tracks = [piano_track, guitar_track, bass_track, strings_track, drums_track])\n",
        "generated_multitrack.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JDyE_5YxJalR",
        "outputId": "7e096edc-04dc-4061-9f23-cc8bbbb02cf5"
      },
      "outputs": [],
      "source": [
        "generated_pm = pypianoroll.to_pretty_midi(generated_multitrack)\n",
        "generated_midi_audio = generated_pm.fluidsynth()\n",
        "IPython.display.Audio(generated_midi_audio, rate = 44100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ahS6ewSLP-4l"
      },
      "outputs": [],
      "source": [
        "generated_path = os.path.join(root_dir, 'Generated MIDIs', 'vae_mj.mid')\n",
        "pypianoroll.write(generated_path, generated_multitrack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEgvFwUuUopN"
      },
      "outputs": [],
      "source": [
        "# Generate VAE (old function)\n",
        "\n",
        "def generate_samples_vae(samples, instrument_vae, noise_sd):\n",
        "  # Expects samples: batch_size x 32 x 128\n",
        "  batch_size = samples.size(0)\n",
        "  # Convert samples array into latent vectors\n",
        "  latent = instrument_vae.infer(samples.to(device))[:, :-1]\n",
        "  # Add some random noise\n",
        "  random_noise = torch.randn_like(latent) * noise_sd\n",
        "  latent_after_noise = latent + random_noise\n",
        "  # Convert latent samples into array\n",
        "  generated_samples = instrument_vae.generate(latent_after_noise.unsqueeze(0)).squeeze(0).view(batch_size, 32, 128)\n",
        "  return generated_samples"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Applying VAE.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
