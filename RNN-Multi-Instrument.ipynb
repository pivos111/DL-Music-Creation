{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWReWjdRjQ6P"
      },
      "source": [
        "# Multi-Instrument RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfcYQM4qjTis"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import pretty_midi\n",
        "import pypianoroll\n",
        "import tables\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "import music21\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "import json\n",
        "import IPython.display\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import random\n",
        "import itertools\n",
        "root_dir = 'drive/MyDrive/ProjectMusic'\n",
        "data_dir = root_dir + '/Lakh Piano Dataset/LPD-5/lpd_5/lpd_5_cleansed'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p_6tH7-02U1",
        "outputId": "57d28b65-3061-403f-aff1-e4484dc7918c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMex7xeZoaor",
        "outputId": "a2484006-1323-48db-cf50-e3a849d07afc"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -qU pyfluidsynth pretty_midi\n",
        "!pip install music21\n",
        "!pip install pypianoroll"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhR5l1SjNTa5"
      },
      "source": [
        "**Getting MIDI and Song Metadata**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng13lLDrN8Za"
      },
      "outputs": [],
      "source": [
        "RESULTS_PATH = os.path.join(root_dir, 'Lakh Piano Dataset', 'Metadata')\n",
        "\n",
        "# Utility functions for retrieving paths\n",
        "def msd_id_to_dirs(msd_id):\n",
        "    \"\"\"Given an MSD ID, generate the path prefix.\n",
        "    E.g. TRABCD12345678 -> A/B/C/TRABCD12345678\"\"\"\n",
        "    return os.path.join(msd_id[2], msd_id[3], msd_id[4], msd_id)\n",
        "\n",
        "\n",
        "def msd_id_to_h5(msd_id):\n",
        "    \"\"\"Given an MSD ID, return the path to the corresponding h5\"\"\"\n",
        "    return os.path.join(RESULTS_PATH, 'lmd_matched_h5',\n",
        "                        msd_id_to_dirs(msd_id) + '.h5')\n",
        "\n",
        "# Load the midi npz file from the LMD cleansed folder\n",
        "def get_midi_npz_path(msd_id, midi_md5):\n",
        "    return os.path.join(data_dir,\n",
        "                        msd_id_to_dirs(msd_id), midi_md5 + '.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY-AyplDYXSm",
        "outputId": "14256d90-a1aa-48f2-aa0f-c8d178de538a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\video\\Desktop\\Project_Deep_Learning\\Our_Project\\RNN-Multi-Instrument.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/video/Desktop/Project_Deep_Learning/Our_Project/RNN-Multi-Instrument.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39m# Open the cleansed ids - cleansed file ids : msd ids\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/video/Desktop/Project_Deep_Learning/Our_Project/RNN-Multi-Instrument.ipynb#ch0000007?line=1'>2</a>\u001b[0m cleansed_ids \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root_dir, \u001b[39m'\u001b[39m\u001b[39mLakh Piano Dataset\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcleansed_ids.txt\u001b[39m\u001b[39m'\u001b[39m), delimiter \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m    \u001b[39m\u001b[39m'\u001b[39m, header \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/video/Desktop/Project_Deep_Learning/Our_Project/RNN-Multi-Instrument.ipynb#ch0000007?line=2'>3</a>\u001b[0m lpd_to_msd_ids \u001b[39m=\u001b[39m {a:b \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(cleansed_ids[\u001b[39m0\u001b[39m], cleansed_ids[\u001b[39m1\u001b[39m])}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/video/Desktop/Project_Deep_Learning/Our_Project/RNN-Multi-Instrument.ipynb#ch0000007?line=3'>4</a>\u001b[0m msd_to_lpd_ids \u001b[39m=\u001b[39m {a:b \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(cleansed_ids[\u001b[39m1\u001b[39m], cleansed_ids[\u001b[39m0\u001b[39m])}\n",
            "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# Open the cleansed ids - cleansed file ids : msd ids\n",
        "cleansed_ids = pd.read_csv(os.path.join(root_dir, 'Lakh Piano Dataset', 'cleansed_ids.txt'), delimiter = '    ', header = None)\n",
        "lpd_to_msd_ids = {a:b for a, b in zip(cleansed_ids[0], cleansed_ids[1])}\n",
        "msd_to_lpd_ids = {a:b for a, b in zip(cleansed_ids[1], cleansed_ids[0])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGhaBVSOzweZ"
      },
      "outputs": [],
      "source": [
        "# Reading the genre annotations\n",
        "genre_file_dir = os.path.join(root_dir, 'Lakh Piano Dataset', 'Genre', 'msd_tagtraum_cd1.cls')\n",
        "ids = []\n",
        "genres = []\n",
        "with open(genre_file_dir) as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "        if line[0] != '#':\n",
        "          split = line.strip().split(\"\\t\")\n",
        "          if len(split) == 2:\n",
        "            ids.append(split[0])\n",
        "            genres.append(split[1])\n",
        "          elif len(split) == 3:\n",
        "            ids.append(split[0])\n",
        "            ids.append(split[0])\n",
        "            genres.append(split[1])\n",
        "            genres.append(split[2])\n",
        "        line = f.readline()\n",
        "genre_df = pd.DataFrame(data={\"TrackID\": ids, \"Genre\": genres})\n",
        "\n",
        "genre_dict = genre_df.groupby('TrackID')['Genre'].apply(lambda x: x.tolist()).to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Le14xf4NVd"
      },
      "source": [
        "**Objects that we need**\n",
        "\n",
        "- cleansed_ids: dictionary of LPD file name : MSD file name\n",
        "- lmd_metadata: list of dictionaries - each dict has a msd_id field to identify\n",
        "- Get the lmd_file_name (actual path )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOeVzvnwV2ht"
      },
      "outputs": [],
      "source": [
        "# Load the processed metadata\n",
        "with open(os.path.join(root_dir, 'Lakh Piano Dataset', 'processed_metadata.json'), 'r') as outfile:\n",
        "  lmd_metadata = json.load(outfile)\n",
        "\n",
        "# Change this into a dictionary of MSD_ID: metadata\n",
        "lmd_metadata = {e['msd_id']:e for e in lmd_metadata}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU0th4Rt5ZMu"
      },
      "outputs": [],
      "source": [
        "# Get all song MSD IDs in pop rock genre\n",
        "rock_song_msd_ids = [k for k, v in lmd_metadata.items() if 'rock' in v['artist_terms']]\n",
        "\n",
        "# Randomly choose 1000 songs out of these\n",
        "#train_ids = random.choices(rock_song_msd_ids, k = 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q8VympnxgDb"
      },
      "outputs": [],
      "source": [
        "rock_song_msd_ids = sorted(rock_song_msd_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xM2flv96PUg"
      },
      "outputs": [],
      "source": [
        "combined_pianorolls = []\n",
        "i = 0\n",
        "for msd_file_name in rock_song_msd_ids[5000:]:\n",
        "\n",
        "  lpd_file_name = msd_to_lpd_ids[msd_file_name]\n",
        "  # Get the NPZ path\n",
        "  npz_path = get_midi_npz_path(msd_file_name, lpd_file_name)\n",
        "  multitrack = pypianoroll.load(npz_path)\n",
        "  multitrack.set_resolution(2).pad_to_same()\n",
        "\n",
        "  # Piano, Guitar, Bass, Strings, Drums\n",
        "  # Splitting into different parts\n",
        "\n",
        "  parts = {'piano_part': None, 'guitar_part': None, 'bass_part': None, 'strings_part': None, 'drums_part': None}\n",
        "  song_length = None\n",
        "  empty_array = None\n",
        "  has_empty_parts = False\n",
        "  for track in multitrack.tracks:\n",
        "    if track.name == 'Drums':\n",
        "      parts['drums_part'] = track.pianoroll\n",
        "    if track.name == 'Piano':\n",
        "      parts['piano_part'] = track.pianoroll\n",
        "    if track.name == 'Guitar':\n",
        "      parts['guitar_part'] = track.pianoroll\n",
        "    if track.name == 'Bass':\n",
        "      parts['bass_part'] = track.pianoroll\n",
        "    if track.name == 'Strings':\n",
        "      parts['strings_part'] = track.pianoroll\n",
        "    if track.pianoroll.shape[0] > 0:\n",
        "      empty_array = np.zeros_like(track.pianoroll)\n",
        "\n",
        "  for k,v in parts.items():\n",
        "    if v.shape[0] == 0:\n",
        "      parts[k] = empty_array.copy()\n",
        "      has_empty_parts = True\n",
        "\n",
        "  # Stack all together - Piano, Guitar, Bass, Strings, Drums\n",
        "  combined_pianoroll = torch.tensor([parts['piano_part'], parts['guitar_part'], parts['bass_part'], parts['strings_part'], parts['drums_part']])\n",
        "\n",
        "  # These contain velocity information - the force with which the notes are hit - which can be standardized to 0/1 if we want (to compress)\n",
        "  if has_empty_parts == False:\n",
        "    combined_pianorolls.append(combined_pianoroll)\n",
        "  i+=1\n",
        "\n",
        "  if (i+1) % 100 == 0:\n",
        "    print(i, datetime.now())\n",
        "\n",
        "  if (i+1) % 1000 == 0:\n",
        "    pianoroll_lengths = [e.size()[1] for e in combined_pianorolls]\n",
        "    out = torch.hstack(combined_pianorolls)\n",
        "    torch.save(out, os.path.join(root_dir, 'Lakh Piano Dataset', 'rock_pianorolls_res2_part1.pt'))\n",
        "    pianoroll_lengths = torch.tensor(pianoroll_lengths)\n",
        "    torch.save(pianoroll_lengths, os.path.join(root_dir, 'Lakh Piano Dataset', 'rock_pianorolls_lengths_res2_part1.pt'))\n",
        "    del out\n",
        "    print('Saved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TeycNi6YyCm"
      },
      "outputs": [],
      "source": [
        "pianoroll_lengths = [e.size()[1] for e in combined_pianorolls]\n",
        "combined_pianorolls = torch.hstack(combined_pianorolls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH0KCy1XZFC9"
      },
      "outputs": [],
      "source": [
        "torch.save(combined_pianorolls, os.path.join(root_dir, 'Lakh Piano Dataset', 'metal_1000_pianorolls.pt'))\n",
        "pianoroll_lengths = torch.tensor(pianoroll_lengths)\n",
        "torch.save(pianoroll_lengths, os.path.join(root_dir, 'Lakh Piano Dataset', 'metal_1000_pianorolls_lengths.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wd2LnhQ-z7w"
      },
      "outputs": [],
      "source": [
        "# Loading\n",
        "combined_pianorolls = torch.load(os.path.join(root_dir, 'Lakh Piano Dataset', 'rock_1000_pianorolls.pt'))\n",
        "pianoroll_lengths = torch.load(os.path.join(root_dir, 'Lakh Piano Dataset', 'rock_1000_pianorolls_lengths.pt'))\n",
        "pianoroll_lengths = pianoroll_lengths.numpy()\n",
        "pianoroll_cum_lengths = pianoroll_lengths.cumsum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n49U3aXeavYV"
      },
      "outputs": [],
      "source": [
        "# Normalize\n",
        "combined_pianorolls = combined_pianorolls / 127.0\n",
        "\n",
        "### Getting the number of notes played in that time step\n",
        "# Number of notes per time step per track\n",
        "notes_per_time_step = (combined_pianorolls > 0).type(torch.float32).sum(axis = 2)\n",
        "# Censor those with more than 10 notes to be 10\n",
        "notes_per_time_step[notes_per_time_step > 10] = 10\n",
        "# Normalize to be between [0, 4] - very important to get right\n",
        "notes_per_time_step = notes_per_time_step / 2\n",
        "notes_per_time_step = notes_per_time_step.unsqueeze(2)\n",
        "# Concatenate the number vector\n",
        "combined_pianorolls = torch.cat((combined_pianorolls, notes_per_time_step), dim = 2)\n",
        "\n",
        "# Remake the list of pianorolls\n",
        "pianorolls_list = []\n",
        "pianorolls_list.append(combined_pianorolls[:, :pianoroll_cum_lengths[0], :])\n",
        "for i in range(len(pianoroll_cum_lengths) - 1):\n",
        "  pianoroll = combined_pianorolls[:, pianoroll_cum_lengths[i]:pianoroll_cum_lengths[i+1], :]\n",
        "  pianoroll = pianoroll[:, ::2, :]\n",
        "  pianorolls_list.append(pianoroll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r82cg5RvbJSy"
      },
      "source": [
        "**Creating Music Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3lZ7cHoGMcW"
      },
      "outputs": [],
      "source": [
        "# Creating dataset and dataloader\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZxOyk_NGTCE"
      },
      "outputs": [],
      "source": [
        "class MusicDataset(Dataset):\n",
        "  def __init__(self, list_of_sequences, dataset_length = 32 * 10000, seq_length = 50):\n",
        "\n",
        "    # Don't normalize anymore since it was done earlier\n",
        "\n",
        "    self.data = list_of_sequences\n",
        "    self.n_songs = len(list_of_sequences)\n",
        "    self.seq_length = seq_length\n",
        "    self.length = dataset_length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    # Choose a random song id\n",
        "    song_id = random.randint(0, self.n_songs - 1)\n",
        "    song_length = self.data[song_id].size()[1]\n",
        "\n",
        "    # Choose a random start window\n",
        "    start_time = random.randint(0, song_length - self.seq_length - 2)\n",
        "    train_sequence = self.data[song_id][:, start_time:(start_time + self.seq_length), :]\n",
        "    #target_sequence = self.data[song_id][:, (start_time + 1):(start_time + self.seq_length + 1), :]\n",
        "\n",
        "    # Target is the next note\n",
        "    target_sequence = self.data[song_id][:, (start_time + self.seq_length + 1), :]\n",
        "    #print(song_id, start_time)\n",
        "    return train_sequence, target_sequence\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA16WwkYC9sp"
      },
      "source": [
        "**RNN Generation Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_7gmx5DPbzA"
      },
      "outputs": [],
      "source": [
        "def grad_clipping(net, theta):  \n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "    \n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad[:] *= theta / norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsLm3d04x-us"
      },
      "outputs": [],
      "source": [
        "model = MultitrackGenerationRNN(input_size = 645, hidden_size = 100, output_size = 645, n_layers = 1).to(device)\n",
        "hidden = model.init_hidden(batch_size = 32)\n",
        "output, hidden = model(train.to(device), hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y7H3n-MDBrM"
      },
      "outputs": [],
      "source": [
        "class MultitrackGenerationRNN(nn.Module):\n",
        "  # input_size: number of possible pitches\n",
        "  # hidden_size: embedding size of each pitch\n",
        "  # output_size: number of possible pitches (probability distribution)\n",
        "    def __init__(self, input_size, hidden_size, output_size, batch_size = 32, n_layers=1):\n",
        "        super(MultitrackGenerationRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # input_size: tracks x pitches\n",
        "        # hidden_size: whatever we want\n",
        "        #self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        #self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "\n",
        "        self.gru = nn.GRU(input_size, hidden_size, n_layers)\n",
        "        self.linear = nn.Linear(hidden_size * n_layers, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "\n",
        "        # Input: batch_size x n_tracks x seq_length x pitches\n",
        "        input = input.permute(0,2,1,3)\n",
        "        # batch x seq_length x track x pitches\n",
        "        input = input.flatten(2,3) # Flatten the track and pitches together\n",
        "        # batch x seq_length x (track x pitches)\n",
        "        # input = self.embedding(input)\n",
        "        # # batch x seq_length x hidden_dim\n",
        "        input = input.permute(1,0,2)\n",
        "        # seq length x batch x hidden_dim\n",
        "        _, hidden = self.gru(input, hidden)\n",
        "\n",
        "        # Hidden: hidden layer at FINAL state\n",
        "        # hidden dim: (num_layer x num_dir) x batch x hidden_size\n",
        "        # **Difference between GRU and LSTMs\n",
        "        h_n = hidden.permute(1,0,2)\n",
        "        # h_n is batch x (num_layer x num_dir) x hidden_size\n",
        "        h_n = h_n.contiguous().flatten(1,2)\n",
        "        # After flattening: batch x (num_layer x num_dir x hidden_size)\n",
        "        output = self.linear(h_n)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbvitNzSuq7J"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, model, optimizer, criterion):\n",
        "  \n",
        "    running_loss = 0\n",
        "    n_obs = 0\n",
        "    # For each character in our chunk (except last), compute the hidden and ouput\n",
        "    # Using each output, compute the loss with the corresponding target \n",
        "    for train_seq, target_seq in dataloader:\n",
        "      train_seq = train_seq.to(device)\n",
        "      target_seq = target_seq.to(device)\n",
        "      hidden = model.init_hidden(batch_size = 32)\n",
        "      optimizer.zero_grad()\n",
        "      target_seq = target_seq.flatten(1,2)\n",
        "      output, hidden = model(train_seq, hidden)\n",
        "      loss = criterion(output, target_seq)\n",
        "      # Backpropagate, clip gradient and optimize\n",
        "      loss.backward()\n",
        "      grad_clipping(model, 1)\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      n_obs += train_seq.size()[0]\n",
        "\n",
        "    # Return average loss for the input sequence\n",
        "    return running_loss / n_obs * 100\n",
        "\n",
        "def test_epoch(dataloader, model, optimizer, criterion):\n",
        "    \n",
        "    running_loss = 0\n",
        "    n_obs = 0\n",
        "    # For each character in our chunk (except last), compute the hidden and ouput\n",
        "    # Using each output, compute the loss with the corresponding target \n",
        "    for train_seq, target_seq in dataloader:\n",
        "      train_seq = train_seq.to(device)\n",
        "      target_seq = target_seq.to(device)\n",
        "      hidden = model.init_hidden(batch_size = 32)\n",
        "      target_seq = target_seq.flatten(1,2)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output, hidden = model(train_seq, hidden)\n",
        "      loss = criterion(output, target_seq)\n",
        "      running_loss += loss.item()\n",
        "      n_obs += train_seq.size()[0]\n",
        "\n",
        "    # Return average loss for the input sequence\n",
        "    return running_loss / n_obs * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs4ZFJ8Du3x-"
      },
      "outputs": [],
      "source": [
        "# Overall training loop\n",
        "def training_loop(model, optimizer, scheduler, criterion, train_dataloader, test_dataloader):\n",
        "\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    print(scheduler.get_last_lr())\n",
        "    train_epoch_loss = train_epoch(train_dataloader, model, optimizer, criterion)\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    test_epoch_loss = test_epoch(test_dataloader, model, optimizer, criterion)\n",
        "    test_losses.append(test_epoch_loss)\n",
        "\n",
        "    print('Epoch {}, Train Loss: {}, Test Loss: {}, Time: {}'.format(epoch, train_epoch_loss, test_epoch_loss, datetime.now()))\n",
        "\n",
        "  return train_losses, test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw2JxeThxz_y"
      },
      "outputs": [],
      "source": [
        "experiment_params = {'SEQ_LENGTH': 100, 'HIDDEN_SIZE': 512, 'N_LAYERS': 3, 'LR_LAMBDA': 0.98}\n",
        "\n",
        "train_dataset = MusicDataset(pianorolls_list[0:900], dataset_length = 32 * 1000, seq_length = experiment_params['SEQ_LENGTH'])\n",
        "train_loader = DataLoader(train_dataset, batch_size = 32, \n",
        "                                    drop_last=True)\n",
        "test_dataset = MusicDataset(pianorolls_list[900:1000], dataset_length = 32 * 500, seq_length = experiment_params['SEQ_LENGTH'])\n",
        "test_loader = DataLoader(test_dataset, batch_size = 32, \n",
        "                                    drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tTzpCEtc5VDf",
        "outputId": "88968d38-ee24-4087-835e-6bed1004e104"
      },
      "outputs": [],
      "source": [
        "# Running the training loop\n",
        "experiment_params = {'SEQ_LENGTH': 100, 'HIDDEN_SIZE': 512, 'N_LAYERS': 3, 'LR_LAMBDA': 1}\n",
        "\n",
        "hidden_size = experiment_params['HIDDEN_SIZE']\n",
        "n_layers = experiment_params['N_LAYERS']\n",
        "n_epochs = 100\n",
        "lr = 0.001\n",
        "lr_lambda = experiment_params['LR_LAMBDA']\n",
        "\n",
        "train_dataset = MusicDataset(pianorolls_list[0:900], dataset_length = 32 * 1000, seq_length = experiment_params['SEQ_LENGTH'])\n",
        "train_loader = DataLoader(train_dataset, batch_size = 32, \n",
        "                                    drop_last=True)\n",
        "test_dataset = MusicDataset(pianorolls_list[900:1000], dataset_length = 32 * 500, seq_length = experiment_params['SEQ_LENGTH'])\n",
        "test_loader = DataLoader(test_dataset, batch_size = 32, \n",
        "                                    drop_last=True)\n",
        "\n",
        "# Create model, optimizer and loss function\n",
        "model = MultitrackGenerationRNN(input_size = 645, hidden_size = hidden_size, output_size = 645, n_layers = n_layers).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)\n",
        "criterion = nn.MSELoss()\n",
        "train_losses, test_losses = training_loop(model, optimizer, scheduler, criterion, train_loader, test_loader)\n",
        "\n",
        "# Plot the losses over epochs\n",
        "plt.plot(train_losses, label = 'Train Loss')\n",
        "plt.plot(test_losses, label = 'Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsPOnZ0y4R4L"
      },
      "outputs": [],
      "source": [
        "model_name = 'RNN with number 26 Apr 3'\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "torch.save(model.state_dict(), save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Uej9i--iPN9t",
        "outputId": "a8c5abb5-a913-4995-f481-33dfbdc7d5c7"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning loop\n",
        "\n",
        "#{'SEQ_LENGTH': 100, 'HIDDEN_SIZE': 512, 'N_LAYERS': 3, 'LR_LAMBDA': 0.98},\n",
        "experiment_params_list = [\n",
        "                     {'SEQ_LENGTH': 200, 'HIDDEN_SIZE': 512, 'N_LAYERS': 5, 'LR_LAMBDA': 0.97},\n",
        "                     {'SEQ_LENGTH': 100, 'HIDDEN_SIZE': 512, 'N_LAYERS': 4, 'LR_LAMBDA': 0.98},\n",
        "                     {'SEQ_LENGTH': 100, 'HIDDEN_SIZE': 512, 'N_LAYERS': 3, 'LR_LAMBDA': 0.96},\n",
        "                     {'SEQ_LENGTH': 100, 'HIDDEN_SIZE': 512, 'N_LAYERS': 3, 'LR_LAMBDA': 0.99},\n",
        "                     {'SEQ_LENGTH': 100, 'HIDDEN_SIZE': 256, 'N_LAYERS': 3, 'LR_LAMBDA': 0.98}]\n",
        "\n",
        "experiment_num = 1\n",
        "for experiment_params in experiment_params_list:                    \n",
        "\n",
        "  print(experiment_num, experiment_params)\n",
        "  hidden_size = experiment_params['HIDDEN_SIZE']\n",
        "  n_layers = experiment_params['N_LAYERS']\n",
        "  n_epochs = 100\n",
        "  lr = 0.001\n",
        "  lr_lambda = experiment_params['LR_LAMBDA']\n",
        "\n",
        "  train_dataset = MusicDataset(combined_pianorolls[0:900], dataset_length = 32 * 1000, seq_length = experiment_params['SEQ_LENGTH'])\n",
        "  train_loader = DataLoader(train_dataset, batch_size = 32, \n",
        "                                      drop_last=True)\n",
        "  test_dataset = MusicDataset(combined_pianorolls[900:1000], dataset_length = 32 * 500, seq_length = experiment_params['SEQ_LENGTH'])\n",
        "  test_loader = DataLoader(test_dataset, batch_size = 32, \n",
        "                                      drop_last=True)\n",
        "\n",
        "  # Create model, optimizer and loss function\n",
        "  model = MultitrackGenerationRNN(input_size = 640, hidden_size = hidden_size, output_size = 640, n_layers = n_layers).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)\n",
        "  criterion = nn.MSELoss()\n",
        "  train_losses, test_losses = training_loop(model, optimizer, scheduler, criterion, train_loader, test_loader)\n",
        "\n",
        "  # Plot the losses over epochs\n",
        "  plt.plot(train_losses, label = 'Train Loss')\n",
        "  plt.plot(test_losses, label = 'Test Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  model_name = 'RNN 21 Apr New Overnight exp{}'.format(experiment_num)\n",
        "  save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "  torch.save(model.state_dict(), save_path)\n",
        "  experiment_num += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRR5QHBVdMuI"
      },
      "outputs": [],
      "source": [
        "experiment_num = 3\n",
        "model_name = 'RNN 20 Apr New Overnight exp{}'.format(experiment_num)\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "torch.save(model.state_dict(), save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB57eIby-Ar7"
      },
      "source": [
        "**Loading Saved Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPfDL-Gp-Czv",
        "outputId": "00c84c19-b62c-4283-a8fc-326b57751933"
      },
      "outputs": [],
      "source": [
        "hidden_size = 512\n",
        "n_layers = 5\n",
        "\n",
        "train_dataset = MusicDataset(combined_pianorolls[0:900], dataset_length = 32 * 1000, seq_length = 100)\n",
        "train_loader = DataLoader(train_dataset, batch_size = 32, \n",
        "                                      drop_last=True)\n",
        "model_name = 'RNN 21 Apr New Overnight exp1'\n",
        "save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "model = MultitrackGenerationRNN(input_size = 640, hidden_size = hidden_size, output_size = 640, n_layers = n_layers)\n",
        "model.load_state_dict(torch.load(save_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2NaHGDnAB_i"
      },
      "outputs": [],
      "source": [
        "train, target = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aRWooUCPhMw",
        "outputId": "ef5387d7-8c62-40c8-ddf7-0493d521c3f9"
      },
      "outputs": [],
      "source": [
        "hidden = model.init_hidden(batch_size = 32)\n",
        "predicted, hidden = model(train.to(device), hidden)\n",
        "\n",
        "predict_len = 200\n",
        "temperature = 12\n",
        "\n",
        "prime_seq = train[30, :, 20:30, :].to(device)\n",
        "hidden = model.init_hidden(batch_size = 1)\n",
        "\n",
        "# Instantiate new tensor to store predicted sequences: n_tracks x seq_length x pitches (128)\n",
        "predictions = torch.zeros((5, predict_len + prime_seq.size()[1], 128)).to(device)\n",
        "\n",
        "# Set the start of the predicted seq to be the prime sequence\n",
        "predictions[:, 0:prime_seq.size()[1], :] = prime_seq[:, :, :128]\n",
        "curr_predict_id = prime_seq.size()[1]\n",
        "\n",
        "prime_seq = prime_seq.unsqueeze(0)\n",
        "\n",
        "# Build up the hidden state\n",
        "_, hidden = model(prime_seq, hidden)\n",
        "\n",
        "# prime_seq is 1 x 5 x 50 x 129\n",
        "input = prime_seq[:, :, -1:, :]\n",
        "# input is 1 x 5 x 1 x 129\n",
        "\n",
        "scores, hidden = model(input, hidden)\n",
        "# predicted is 1 x 645\n",
        "scores = scores.view(5, 129)\n",
        "predicted_n_notes = (scores[:, -1] * 2 + 1).type(torch.int8)\n",
        "\n",
        "print(predicted_n_notes)\n",
        "scores = scores[:, :-1]\n",
        "\n",
        "scores = scores / predicted.max()\n",
        "predicted_probs = F.softmax(scores * temperature, dim = 1)\n",
        "predicted = torch.zeros_like(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4KG061QHGbK"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "instrument_predicted_n_notes = predicted_n_notes[i].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NU8gOtKHKK9"
      },
      "outputs": [],
      "source": [
        "topk, indices = torch.topk(predicted_probs[i, :], instrument_predicted_n_notes * 2) # Twice the number of notes up for selection\n",
        "instrument_predicted_indices_ids = torch.multinomial(topk, num_samples = instrument_predicted_n_notes)\n",
        "instrument_predicted_ids = torch.gather(indices, 0, instrument_predicted_indices_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hi7b93WRIIzI"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in range(5):\n",
        "  instrument_predicted_n_notes = predicted_n_notes[i].item() #  Get number of predicted notes for that instrument\n",
        "  if instrument_predicted_n_notes > 0:\n",
        "    topk, indices = torch.topk(predicted_probs[i, :], instrument_predicted_n_notes * 2) # Twice the number of notes up for selection\n",
        "    instrument_predicted_indices_ids = torch.multinomial(topk, num_samples = instrument_predicted_n_notes)\n",
        "    instrument_predicted_ids = torch.gather(indices, 0, instrument_predicted_indices_ids)\n",
        "    predicted[i, instrument_predicted_ids] = 1 # Set the predicted notes ids to 1\n",
        "\n",
        "# Set next input to just generated prediction\n",
        "input = predicted.clone()\n",
        "# Get number of notes for generated prediction\n",
        "new_predicted_n_notes = (predicted > 0).sum(axis = 1) / 10\n",
        "new_predicted_n_notes = new_predicted_n_notes.unsqueeze(1)\n",
        "input = torch.cat((input, new_predicted_n_notes), dim = 1)\n",
        "\n",
        "predictions[:, curr_predict_id, :] = predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed3pMBmwHc_4"
      },
      "outputs": [],
      "source": [
        "# Code to evaluate the language model i.e. generate new music\n",
        "# Old code that only generates a fixed number of notes per instrument at any time\n",
        "\n",
        "def evaluate(net, prime_seq, predict_len = 100, temperature = 20):\n",
        "    '''\n",
        "    Arguments:\n",
        "    prime_seq - priming sequence (converted t)\n",
        "    predict_len - number of notes to predict for after prime sequence\n",
        "    '''\n",
        "    hidden = net.init_hidden(batch_size = 1)\n",
        "\n",
        "    # Instantiate new tensor to store predicted sequences\n",
        "    predictions = torch.zeros((5, predict_len + prime_seq.size()[1], 128)).to(device)\n",
        "\n",
        "    # Set the start of the predicted seq to be the prime sequence\n",
        "    predictions[:, 0:prime_seq.size()[1], :] = prime_seq[:, :, :128]\n",
        "\n",
        "    curr_predict_id = prime_seq.size()[1]\n",
        "\n",
        "    # Reshape prime seq \n",
        "    # from n_tracks x seq_length x pitches\n",
        "    # to become batch_size x n_tracks x seq_length x pitches\n",
        "    prime_seq = prime_seq.unsqueeze(0)\n",
        "\n",
        "    # Build up the hidden state\n",
        "    _, hidden = net(prime_seq, hidden)\n",
        "    # Input is last character of prime sequence\n",
        "    input = predictions[:, prime_seq.size()[1] - 1, :]\n",
        "\n",
        "    while curr_predict_id < predictions.size()[1]:\n",
        "      print('input before', input.size())\n",
        "      # Forward pass of the trained NN - to get next predicted front\n",
        "      input = input.unsqueeze(0).unsqueeze(2)\n",
        "      print('input after', input.size())\n",
        "      predicted, hidden = net(input, hidden)\n",
        "      predicted = predicted / predicted.max()\n",
        "      predicted = predicted.view(5, 128)\n",
        "\n",
        "      predicted_probs = F.softmax(predicted * temperature, dim = 1)\n",
        "      predicted_probs[predicted_probs < 0.001] = 0.0\n",
        "      predicted_ids = torch.multinomial(predicted_probs, num_samples = 3)\n",
        "      predicted = torch.zeros_like(predicted)\n",
        "\n",
        "      for i in range(5):\n",
        "        predicted[i, predicted_ids[i]] = 1\n",
        "\n",
        "      # Bernoulli randomly generate based on the probabilities\n",
        "      #predicted = torch.bernoulli(predicted)\n",
        "      input = predicted.clone()\n",
        "      predictions[:, curr_predict_id, :] = predicted\n",
        "      \n",
        "      curr_predict_id += 1\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1VyBGB-DDsf"
      },
      "outputs": [],
      "source": [
        "# Code to evaluate the language model i.e. generate new music\n",
        "# New code that takes in the encoding of how many notes at every time step\n",
        "\n",
        "def evaluateWithNumber(net, prime_seq, predict_len = 100, temperature = 20):\n",
        "    '''\n",
        "    Arguments:\n",
        "    prime_seq - priming sequence (converted t)\n",
        "    predict_len - number of notes to predict for after prime sequence\n",
        "    '''\n",
        "    hidden = model.init_hidden(batch_size = 1)\n",
        "\n",
        "    # Instantiate new tensor to store predicted sequences: n_tracks x seq_length x pitches (128)\n",
        "    predictions = torch.zeros((5, predict_len + prime_seq.size()[1], 128)).to(device)\n",
        "    predictions_n_notes = torch.zeros((5, predict_len + prime_seq.size()[1]))\n",
        "\n",
        "    # Set the start of the predicted seq to be the prime sequence\n",
        "    predictions[:, 0:prime_seq.size()[1], :] = prime_seq[:, :, :128]\n",
        "    curr_predict_id = prime_seq.size()[1]\n",
        "\n",
        "    prime_seq = prime_seq.unsqueeze(0)\n",
        "\n",
        "    # Build up the hidden state\n",
        "    _, hidden = model(prime_seq, hidden)\n",
        "\n",
        "    # prime_seq is 1 x 5 x 50 x 129\n",
        "    input = prime_seq[:, :, -1:, :]\n",
        "    # input is 1 x 5 x 1 x 129\n",
        "\n",
        "    while curr_predict_id < predictions.size()[1]:\n",
        "      \n",
        "      scores, hidden = model(input, hidden)\n",
        "      # predicted is 1 x 645\n",
        "      scores = scores.view(5, 129)\n",
        "      predicted_n_notes = (scores[:, -1] * 2 + 1).type(torch.int8)\n",
        "      scores = scores[:, :-1]\n",
        "\n",
        "      scores = scores / scores.max()\n",
        "      predicted_probs = F.softmax(scores * temperature, dim = 1)\n",
        "      predicted_probs[predicted_probs < 0.001] = 0.0\n",
        "      predicted = torch.zeros_like(scores)\n",
        "\n",
        "      for i in range(5):\n",
        "        instrument_predicted_n_notes = predicted_n_notes[i].item() #  Get number of predicted notes for that instrument\n",
        "        if instrument_predicted_n_notes > 0:\n",
        "          topk, indices = torch.topk(predicted_probs[i, :], instrument_predicted_n_notes * 2) # Twice the number of notes up for consideration (get the top and ignore the rest)\n",
        "          topk[0] = topk[0] / 2\n",
        "          print(topk / topk.sum())\n",
        "          instrument_predicted_indices_ids = torch.multinomial(topk, num_samples = instrument_predicted_n_notes) # Choose from the multinomial \n",
        "          instrument_predicted_ids = torch.gather(indices, 0, instrument_predicted_indices_ids)\n",
        "          predicted[i, instrument_predicted_ids] = 1 # Set the predicted notes ids to 1\n",
        "\n",
        "      # Set next input to just generated prediction\n",
        "      input = predicted.clone()\n",
        "      # Get number of notes for generated prediction\n",
        "      new_predicted_n_notes = (predicted > 0).sum(axis = 1) / 2\n",
        "      predictions_n_notes[:, curr_predict_id] = new_predicted_n_notes\n",
        "      new_predicted_n_notes = new_predicted_n_notes.unsqueeze(1)\n",
        "      input = torch.cat((input, new_predicted_n_notes), dim = 1).unsqueeze(0).unsqueeze(2)\n",
        "      predictions[:, curr_predict_id, :] = predicted\n",
        "\n",
        "      curr_predict_id += 1\n",
        "\n",
        "    return predictions, predictions_n_notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvieKL2NQ3M1",
        "outputId": "36b17477-dbb2-444d-a56a-49252628d874"
      },
      "outputs": [],
      "source": [
        "# Prime sequence\n",
        "model.to(device)\n",
        "prime_seq = train[14, :, 30:80, :].to(device)\n",
        "predictions, predictions_n_notes = evaluateWithNumber(model, prime_seq, predict_len = 100, temperature = 10)\n",
        "# Unnormalize\n",
        "predictions = (predictions * 127).type(torch.int8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gc0Lu52AfRv"
      },
      "outputs": [],
      "source": [
        "# Prime sequence\n",
        "model.to(device)\n",
        "prime_seq = train[29, :, 10:20, 0:128].to(device)\n",
        "predictions = evaluate(model, prime_seq, predict_len = 200, temperature = 10)\n",
        "# Unnormalize\n",
        "predictions = (predictions * 127).type(torch.int8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UNb-MWw8vpS",
        "outputId": "953cfa70-85f7-4407-dd1d-48bdd47540f8"
      },
      "outputs": [],
      "source": [
        "prime_seq.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "zKV5mgLNI6Ou",
        "outputId": "36c9244f-382f-4992-c854-3cf8d7e11be4"
      },
      "outputs": [],
      "source": [
        "# Convert predictions into the multitrack pianoroll\n",
        "piano_track = pypianoroll.StandardTrack(name = 'Piano', program = 0, is_drum = False, pianoroll = predictions[0, :, :].detach().cpu().numpy())\n",
        "guitar_track = pypianoroll.StandardTrack(name = 'Guitar', program = 24, is_drum = False, pianoroll = predictions[1, :, :].detach().cpu().numpy())\n",
        "bass_track = pypianoroll.StandardTrack(name = 'Bass', program = 32, is_drum = False, pianoroll = predictions[2, :, :].detach().cpu().numpy())\n",
        "strings_track = pypianoroll.StandardTrack(name = 'Strings', program = 48, is_drum = False, pianoroll = predictions[3, :, :].detach().cpu().numpy())\n",
        "drums_track = pypianoroll.StandardTrack(name = 'Drums', is_drum = True, pianoroll = predictions[4, :, :].detach().cpu().numpy())\n",
        "\n",
        "\n",
        "generated_multitrack = pypianoroll.Multitrack(name = 'Generated', resolution = 2, tracks = [piano_track, guitar_track, bass_track, strings_track, drums_track])\n",
        "\n",
        "\n",
        "#resolution=24, tempo=array(shape=(12000,), dtype=float64), downbeat=array(shape=(12000,), dtype=bool)\n",
        "# Plot the generated multitrack\n",
        "generated_multitrack.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "ZC2qwLZQI6SV",
        "outputId": "21cada94-dab3-498c-9c47-d7aa1a5da2e7"
      },
      "outputs": [],
      "source": [
        "# Convert generated multitrack to pretty midi\n",
        "generated_pm = pypianoroll.to_pretty_midi(generated_multitrack)\n",
        "generated_midi_audio = generated_pm.fluidsynth()\n",
        "IPython.display.Audio(generated_midi_audio, rate = 44100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hvHikNW2zAw"
      },
      "outputs": [],
      "source": [
        "generated_path = os.path.join(root_dir, 'Generated MIDIs', 'multitrack_21apr_2.mid')\n",
        "pypianoroll.write(generated_path, generated_multitrack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMYG0q6FC9Gm"
      },
      "outputs": [],
      "source": [
        "def evaluateMultinomial(net, prime_seq, predict_len, temperature=0.8):\n",
        "    '''\n",
        "    Arguments:\n",
        "    prime_seq - priming sequence (converted t)\n",
        "    predict_len - number of notes to predict for after prime sequence\n",
        "    '''\n",
        "    hidden = net.init_hidden()\n",
        "\n",
        "    predicted = prime_seq.copy()\n",
        "    prime_seq = torch.tensor(prime_seq, dtype = torch.long)\n",
        "\n",
        "\n",
        "    # \"Building up\" the hidden state using the prime sequence\n",
        "    for p in range(len(prime_seq) - 1):\n",
        "        input = prime_seq[p]\n",
        "        _, hidden = net(input, hidden)\n",
        "    \n",
        "    # Last character of prime sequence\n",
        "    input = prime_seq[-1]\n",
        "    \n",
        "    # For every index to predict\n",
        "    for p in range(predict_len):\n",
        "\n",
        "        # Pass the inputs to the model - output has dimension n_pitches - scores for each of the possible characters\n",
        "        output, hidden = net(input, hidden)\n",
        "        # Sample from the network output as a multinomial distribution\n",
        "        output = output.data.view(-1).div(temperature).exp()\n",
        "        predicted_id = torch.multinomial(output, 1)\n",
        "\n",
        "        # Add predicted index to the list and use as next input\n",
        "        predicted.append(predicted_id.item()) \n",
        "        input = predicted_id\n",
        "\n",
        "    return predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WVKK0ZO0o2V",
        "outputId": "b9d8b483-82fe-4f6c-bc56-b2698a2a8d46"
      },
      "outputs": [],
      "source": [
        "generated_seq = evaluate(model, [100, 101, 102, 103, 104], predict_len = 100)\n",
        "generated_seq_multinomial = evaluateMultinomial(model, [100, 101, 102, 103, 104], predict_len = 100, temperature = 1)\n",
        "print(generated_seq)\n",
        "print(generated_seq_multinomial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpmwJwn12DVh"
      },
      "source": [
        "**Converting Generated Sequences into MIDI/Audio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHm9B7102Yj2"
      },
      "outputs": [],
      "source": [
        "# Convert the generated ints into notes\n",
        "generated_seq = [int_to_note[e] for e in generated_seq]\n",
        "generated_seq_multinomial = [int_to_note[e] for e in generated_seq_multinomial]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Sj_eqFFC9K4"
      },
      "outputs": [],
      "source": [
        "def create_midi(prediction_output):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    return midi_stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jiXfj3c5C9Nv",
        "outputId": "01a385ad-b86d-4feb-ed4f-0fff1e48e1e5"
      },
      "outputs": [],
      "source": [
        "generated_path = os.path.join(root_dir, 'Generated MIDIs', '19apr_2.mid')\n",
        "generated_stream = create_midi(generated_seq_multinomial)\n",
        "generated_stream.write('midi', fp=generated_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "6_PcJasjC9PP",
        "outputId": "df22d053-a1e9-4dbb-818c-1577244e48ce"
      },
      "outputs": [],
      "source": [
        "# Load the generated MIDI\n",
        "generated_multitrack = pypianoroll.read(generated_path)\n",
        "generated_pm = pypianoroll.to_pretty_midi(generated_multitrack)\n",
        "generated_midi_audio = generated_pm.fluidsynth()\n",
        "IPython.display.Audio(generated_midi_audio, rate = 44100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Multi-Instrument RNN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "83740cbcf812e8702e03da8a51b9f98a79fdb99ef2df29f711ed238884eeacd5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
